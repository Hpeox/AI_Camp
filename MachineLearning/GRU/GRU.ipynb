{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchmetrics\n",
    "torch.manual_seed(0)\n",
    "torch.backends.cudnn.deterministic = False\n",
    "torch.backends.cudnn.benchmark = True\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import tqdm, sys, os, gc, argparse, warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import gc\n",
    "import time\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = \"../data/\"\n",
    "\n",
    "# train_files = os.listdir(path + \"train\")\n",
    "# train_df = pd.DataFrame()\n",
    "# for filename in tqdm.tqdm(train_files):\n",
    "#     tmp = pd.read_csv(path + \"train/\" + filename)\n",
    "#     tmp[\"file\"] = filename\n",
    "#     train_df = pd.concat([train_df, tmp], axis=0, ignore_index=True)\n",
    "\n",
    "# test_files = os.listdir(path + \"test\")\n",
    "# test_df = pd.DataFrame()\n",
    "# for filename in tqdm.tqdm(test_files):\n",
    "#     tmp = pd.read_csv(path + \"test/\" + filename)\n",
    "#     tmp[\"file\"] = filename\n",
    "#     test_df = pd.concat([test_df, tmp], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df[\"tick\"] = (\n",
    "#     (train_df[\"date\"] * 24 + train_df[\"time\"].apply(lambda x: int(x.split(\":\")[0])))\n",
    "#     * 60\n",
    "#     + train_df[\"time\"].apply(lambda x: int(x.split(\":\")[1]))\n",
    "# ) * 20 + train_df[\"time\"].apply(lambda x: int(x.split(\":\")[2])) // 3\n",
    "# test_df[\"tick\"] = (\n",
    "#     (test_df[\"date\"] * 24 + test_df[\"time\"].apply(lambda x: int(x.split(\":\")[0]))) * 60\n",
    "#     + test_df[\"time\"].apply(lambda x: int(x.split(\":\")[1]))\n",
    "# ) * 20 + test_df[\"time\"].apply(lambda x: int(x.split(\":\")[2])) // 3\n",
    "\n",
    "# train_df = train_df.sort_values([\"sym\", \"tick\"])\n",
    "# test_df = test_df.sort_values([\"sym\", \"tick\"])\n",
    "\n",
    "# cols=['file' ,'uuid','n_close' ,'amount_delta', 'n_midprice', 'n_bid1', 'n_bsize1', 'n_bid2', 'n_bsize2', 'n_ask1', 'n_asize1', 'n_ask2', 'n_asize2']\n",
    "# target_df=train_df[['label_5','label_10','label_20','label_40','label_60']]\n",
    "# train_df=train_df[cols]\n",
    "# test_df=test_df[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_df.to_csv('.\\data\\\\target.csv',index=False)\n",
    "# train_df.to_csv('.\\data\\\\train.csv',index=False)\n",
    "# test_df.to_csv('.\\data\\\\test.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df=pd.read_csv('.\\data\\\\train.csv')\n",
    "# target_df=pd.read_csv('.\\data\\\\target.csv')\n",
    "# test_df=pd.read_csv('./data//test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../data/\"\n",
    "train_df=pd.read_csv(path+'train/'+'snapshot_sym0_date0_am.csv')\n",
    "test_df=pd.read_csv(path+'test/'+'snapshot_sym0_date64_am.csv')\n",
    "\n",
    "cols=['n_close' ,'amount_delta', 'n_midprice', 'n_bid1', 'n_bsize1', 'n_bid2', 'n_bsize2', 'n_ask1', 'n_asize1', 'n_ask2', 'n_asize2']\n",
    "target_df=train_df[['label_5','label_10','label_20','label_40','label_60']]\n",
    "train_df=train_df[cols]\n",
    "test_df=test_df[cols]\n",
    "train_df=train_df.astype('float32')\n",
    "test_df=test_df.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length=50\n",
    "tmp=train_df.iloc[[0]]\n",
    "for i in range(seq_length-2):\n",
    "    tmp=tmp.append(train_df.iloc[[0]])\n",
    "train_df=tmp.append(train_df)\n",
    "tmp=test_df.iloc[[0]]\n",
    "for i in range(seq_length-1):\n",
    "    tmp=tmp.append(test_df.iloc[[0]])\n",
    "test_df=tmp.append(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1999, 50, 11])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features=torch.from_numpy(np.array(train_df))\n",
    "# features=features.permute(1,0)\n",
    "# features.size()\n",
    "features = features.unfold(0,50,1)\n",
    "# features.size()\n",
    "features=features.permute(0,2,1)\n",
    "features.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets=torch.from_numpy(np.array(target_df))\n",
    "targets[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_length=50\n",
    "features=torch.tensor([])\n",
    "targets=torch.tensor([])\n",
    "tfeatures=torch.tensor([])\n",
    "for i in range(1999):\n",
    "    f=torch.from_numpy(np.array(train_df.iloc[i:i+seq_length]))\n",
    "    tf=torch.from_numpy(np.array(test_df.iloc[i:i+seq_length]))\n",
    "    t=torch.from_numpy(np.array(target_df.iloc[[i]]))\n",
    "    features=torch.cat((features,torch.unsqueeze(f,0)),0)\n",
    "    tfeatures=torch.cat((tfeatures,torch.unsqueeze(tf,0)),0)\n",
    "    targets=torch.cat((targets,t),0)\n",
    "# features[0]\n",
    "targets[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_index=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqDataset(Dataset):\n",
    "    def __init__(self, data_features, data_targets,target_index):\n",
    "        self.len = data_features.size()[0]\n",
    "        self.features=data_features\n",
    "        self.targets=data_targets[:,target_index]\n",
    "    def __getitem__(self, index):\n",
    "        return self.features[index], self.targets[index]\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "Data_set=SeqDataset(data_features=features,data_targets=targets,target_index=target_index)\n",
    "test_set=SeqDataset(data_features=tfeatures,data_targets=targets,target_index=target_index)\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "    dataset=Data_set, lengths=[0.9, 0.1], generator=torch.Generator().manual_seed(42)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainLoader=torch.utils.data.DataLoader(train_dataset,batch_size=16,shuffle=True,num_workers=0, pin_memory=False)\n",
    "valLoader=torch.utils.data.DataLoader(val_dataset,batch_size=16,shuffle=True,num_workers=0, pin_memory=False)\n",
    "testLoader=torch.utils.data.DataLoader(test_set,batch_size=32,shuffle=True,num_workers=0, pin_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRU\n",
    "class GRU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GRU, self).__init__()\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=len(cols),\n",
    "            hidden_size=64,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(64, 32),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(16, 3),\n",
    "        )\n",
    "    \n",
    "    def forward(self, input):\n",
    "        output, _ = self.gru(input, None)\n",
    "        output = output[:, -1, :]\n",
    "        output = self.mlp(output)\n",
    "        return output\n",
    "\n",
    "model=GRU().to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), 0.0001)\n",
    "criterion = nn.CrossEntropyLoss().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer):\n",
    "    start = time.time()\n",
    "    start_batch = [start, 0]\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    preds = torch.tensor([])\n",
    "    target_all = torch.tensor([])\n",
    "    for i, (input, target) in enumerate(train_loader):\n",
    "        input = input.to(device)\n",
    "        target = target.to(torch.int64).to(device)\n",
    "        output = model(input)\n",
    "        loss = criterion(output, target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (i + 1) % 100 == 0:\n",
    "            start_batch[((i + 1) // 100) % 2] = time.time()\n",
    "            print(\n",
    "                \"Train loss\",\n",
    "                loss.item(),\n",
    "                \"t={}s\".format(\n",
    "                    start_batch[((i + 1) // 100) % 2]\n",
    "                    - start_batch[(1 + (i + 1) // 100) % 2]\n",
    "                ),\n",
    "            )\n",
    "\n",
    "        preds = torch.cat((preds, output.cpu().argmax(1)))\n",
    "        target_all = torch.cat((target_all, target.cpu()))\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    val_acc = torchmetrics.functional.classification.multiclass_f1_score(\n",
    "        preds, target_all, num_classes=25, average=\"macro\"\n",
    "    )\n",
    "    print(i)\n",
    "    print(\"t={}s\".format(time.time() - start))\n",
    "    print(\"F1 score\", val_acc)\n",
    "    return train_loss / len(train_loader)\n",
    "\n",
    "def validate(val_loader, model, criterion):\n",
    "    model.eval()\n",
    "    val_acc = 0.0\n",
    "    preds = torch.tensor([])\n",
    "    target_all = torch.tensor([])\n",
    "    val_loss=0.0\n",
    "    with torch.no_grad():\n",
    "        for i, (input, target) in enumerate(val_loader):\n",
    "            input = input.to(device)\n",
    "            target = target.to(torch.int64).to(device)\n",
    "            output = model(input)\n",
    "            loss = criterion(output, target)\n",
    "            # val_acc += (output.argmax(1) == target).sum().item()\n",
    "            preds = torch.cat((preds, output.cpu().argmax(1)))\n",
    "            target_all = torch.cat((target_all, target.cpu()))\n",
    "\n",
    "        val_acc = torchmetrics.functional.classification.multiclass_f1_score(\n",
    "            preds, target_all, num_classes=25, average=\"macro\"\n",
    "        )\n",
    "        val_loss += loss.item()\n",
    "    # return val_acc / len(val_loader.dataset)\n",
    "    return val_acc,val_loss/len(val_loader)\n",
    "\n",
    "def predict(test_loader, model):\n",
    "    model.eval()\n",
    "\n",
    "    test_pred = []\n",
    "    with torch.no_grad():\n",
    "        for i, (input, _) in enumerate(test_loader):\n",
    "            input = input.to(device)\n",
    "            output = model(input)\n",
    "            test_pred.append(output.data.cpu().numpy())\n",
    "\n",
    "    return np.vstack(test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.4153522849082947 t=0.4659888744354248s\n",
      "112\n",
      "t=0.4960815906524658s\n",
      "F1 score tensor(0.0339)\n",
      "epoch 1 : train_loss: 0.7594163502212119 val_loss: 0.08051744791177604 val_f1: tensor(0.0349)\n",
      "Train loss 0.842707633972168 t=0.23580670356750488s\n",
      "112\n",
      "t=0.2806997299194336s\n",
      "F1 score tensor(0.0339)\n",
      "epoch 2 : train_loss: 0.7602404590729064 val_loss: 0.06137483853560228 val_f1: tensor(0.0349)\n",
      "Train loss 0.531100869178772 t=0.22405719757080078s\n",
      "112\n",
      "t=0.25747036933898926s\n",
      "F1 score tensor(0.0339)\n",
      "epoch 3 : train_loss: 0.7611211406446137 val_loss: 0.06077424837992741 val_f1: tensor(0.0349)\n",
      "Train loss 0.30872175097465515 t=0.22574877738952637s\n",
      "112\n",
      "t=0.2560088634490967s\n",
      "F1 score tensor(0.0339)\n",
      "epoch 4 : train_loss: 0.7597316669679322 val_loss: 0.08009287027212289 val_f1: tensor(0.0349)\n",
      "Train loss 0.8356626629829407 t=0.21640753746032715s\n",
      "112\n",
      "t=0.2416379451751709s\n",
      "F1 score tensor(0.0339)\n",
      "epoch 5 : train_loss: 0.759563180461394 val_loss: 0.06206699059559749 val_f1: tensor(0.0349)\n",
      "Train loss 1.0694341659545898 t=0.22669243812561035s\n",
      "112\n",
      "t=0.25386643409729004s\n",
      "F1 score tensor(0.0339)\n",
      "epoch 6 : train_loss: 0.7584306138806638 val_loss: 0.06061679124832153 val_f1: tensor(0.0349)\n",
      "Train loss 0.5220774412155151 t=0.21853899955749512s\n",
      "112\n",
      "t=0.2520771026611328s\n",
      "F1 score tensor(0.0339)\n",
      "epoch 7 : train_loss: 0.757492688641084 val_loss: 0.0234832213475154 val_f1: tensor(0.0349)\n",
      "Train loss 0.5196695327758789 t=0.22715473175048828s\n",
      "112\n",
      "t=0.2566535472869873s\n",
      "F1 score tensor(0.0339)\n",
      "epoch 8 : train_loss: 0.758188554407221 val_loss: 0.041747450828552246 val_f1: tensor(0.0349)\n",
      "Train loss 0.8448500633239746 t=0.2342534065246582s\n",
      "112\n",
      "t=0.26500630378723145s\n",
      "F1 score tensor(0.0339)\n",
      "epoch 9 : train_loss: 0.7611467964881289 val_loss: 0.04098265904646654 val_f1: tensor(0.0349)\n",
      "Train loss 0.7362820506095886 t=0.22262215614318848s\n",
      "112\n",
      "t=0.2539987564086914s\n",
      "F1 score tensor(0.0339)\n",
      "epoch 10 : train_loss: 0.7589330161567283 val_loss: 0.04263334090893085 val_f1: tensor(0.0349)\n",
      "Train loss 0.7288402915000916 t=0.21601176261901855s\n",
      "112\n",
      "t=0.2463536262512207s\n",
      "F1 score tensor(0.0339)\n",
      "epoch 11 : train_loss: 0.7581365585854624 val_loss: 0.04319135500834538 val_f1: tensor(0.0349)\n",
      "Train loss 0.7298481464385986 t=0.21398425102233887s\n",
      "112\n",
      "t=0.24201250076293945s\n",
      "F1 score tensor(0.0339)\n",
      "epoch 12 : train_loss: 0.7582959617133689 val_loss: 0.08105337619781494 val_f1: tensor(0.0349)\n",
      "Train loss 0.7311545610427856 t=0.2297194004058838s\n",
      "112\n",
      "t=0.2602057456970215s\n",
      "F1 score tensor(0.0339)\n",
      "epoch 13 : train_loss: 0.7577651398899281 val_loss: 0.043451208334702715 val_f1: tensor(0.0349)\n",
      "Train loss 0.8403393626213074 t=0.21866559982299805s\n",
      "112\n",
      "t=0.2484111785888672s\n",
      "F1 score tensor(0.0339)\n",
      "epoch 14 : train_loss: 0.7604176900555603 val_loss: 0.06157522935133714 val_f1: tensor(0.0349)\n",
      "Train loss 0.8410236239433289 t=0.22588157653808594s\n",
      "112\n",
      "t=0.25869083404541016s\n",
      "F1 score tensor(0.0339)\n",
      "epoch 15 : train_loss: 0.7575401220701438 val_loss: 0.061214240697714 val_f1: tensor(0.0349)\n",
      "Train loss 0.6441396474838257 t=0.22987008094787598s\n",
      "112\n",
      "t=0.2616419792175293s\n",
      "F1 score tensor(0.0339)\n",
      "epoch 16 : train_loss: 0.7582235753008749 val_loss: 0.04237862275196956 val_f1: tensor(0.0349)\n",
      "Train loss 0.6342332363128662 t=0.22390031814575195s\n",
      "112\n",
      "t=0.2542612552642822s\n",
      "F1 score tensor(0.0339)\n",
      "epoch 17 : train_loss: 0.7582771983821835 val_loss: 0.1008526270206158 val_f1: tensor(0.0349)\n",
      "Train loss 0.8434291481971741 t=0.2324061393737793s\n",
      "112\n",
      "t=0.2649502754211426s\n",
      "F1 score tensor(0.0339)\n",
      "epoch 18 : train_loss: 0.7565212218107376 val_loss: 0.04081992461131169 val_f1: tensor(0.0349)\n",
      "Train loss 0.5201765894889832 t=0.2249774932861328s\n",
      "112\n",
      "t=0.25615644454956055s\n",
      "F1 score tensor(0.0339)\n",
      "epoch 19 : train_loss: 0.7574166196637449 val_loss: 0.04221209195943979 val_f1: tensor(0.0349)\n",
      "Train loss 0.4295114576816559 t=0.22683930397033691s\n",
      "112\n",
      "t=0.2590053081512451s\n",
      "F1 score tensor(0.0339)\n",
      "epoch 20 : train_loss: 0.7562436830680982 val_loss: 0.059731437609745905 val_f1: tensor(0.0349)\n",
      "Train loss 0.7336511015892029 t=0.2272658348083496s\n",
      "112\n",
      "t=0.2580544948577881s\n",
      "F1 score tensor(0.0339)\n",
      "epoch 21 : train_loss: 0.7582893748726465 val_loss: 0.04137328496346107 val_f1: tensor(0.0349)\n",
      "Train loss 0.5071438550949097 t=0.2223212718963623s\n",
      "112\n",
      "t=0.2516438961029053s\n",
      "F1 score tensor(0.0339)\n",
      "epoch 22 : train_loss: 0.758227001513 val_loss: 0.07892050192906307 val_f1: tensor(0.0349)\n",
      "Train loss 0.9506934285163879 t=0.21894359588623047s\n",
      "112\n",
      "t=0.2480332851409912s\n",
      "F1 score tensor(0.0339)\n",
      "epoch 23 : train_loss: 0.7594868991754752 val_loss: 0.02355513435143691 val_f1: tensor(0.0349)\n",
      "Train loss 0.8418587446212769 t=0.23183631896972656s\n",
      "112\n",
      "t=0.26545286178588867s\n",
      "F1 score tensor(0.0339)\n",
      "epoch 24 : train_loss: 0.7591092043218359 val_loss: 0.042634207468766436 val_f1: tensor(0.0349)\n",
      "Train loss 0.7394772171974182 t=0.22680974006652832s\n",
      "112\n",
      "t=0.25995564460754395s\n",
      "F1 score tensor(0.0339)\n",
      "epoch 25 : train_loss: 0.7568144753443456 val_loss: 0.06241010244076069 val_f1: tensor(0.0349)\n",
      "Train loss 0.6311817765235901 t=0.2259964942932129s\n",
      "112\n",
      "t=0.25671863555908203s\n",
      "F1 score tensor(0.0339)\n",
      "epoch 26 : train_loss: 0.7578952048732116 val_loss: 0.041522475389333874 val_f1: tensor(0.0349)\n",
      "Train loss 0.9171805381774902 t=0.23130202293395996s\n",
      "112\n",
      "t=0.26189756393432617s\n",
      "F1 score tensor(0.0339)\n",
      "epoch 27 : train_loss: 0.7567901231546318 val_loss: 0.042433940447293796 val_f1: tensor(0.0349)\n",
      "Train loss 0.8302245736122131 t=0.23176932334899902s\n",
      "112\n",
      "t=0.25977659225463867s\n",
      "F1 score tensor(0.0339)\n",
      "epoch 28 : train_loss: 0.7558556829406097 val_loss: 0.05958718978441679 val_f1: tensor(0.0349)\n",
      "Train loss 0.5971933603286743 t=0.22323966026306152s\n",
      "112\n",
      "t=0.2559170722961426s\n",
      "F1 score tensor(0.0339)\n",
      "epoch 29 : train_loss: 0.7578042157455883 val_loss: 0.06185630193123451 val_f1: tensor(0.0349)\n",
      "Train loss 0.7369920015335083 t=0.22151660919189453s\n",
      "112\n",
      "t=0.2502937316894531s\n",
      "F1 score tensor(0.0339)\n",
      "epoch 30 : train_loss: 0.7573561955869725 val_loss: 0.022609055042266846 val_f1: tensor(0.0349)\n",
      "Train loss 0.5096791982650757 t=0.217116117477417s\n",
      "112\n",
      "t=0.2480933666229248s\n",
      "F1 score tensor(0.0339)\n",
      "epoch 31 : train_loss: 0.757580279512743 val_loss: 0.07962924700516921 val_f1: tensor(0.0349)\n",
      "Train loss 1.0544767379760742 t=0.23041391372680664s\n",
      "112\n",
      "t=0.2574763298034668s\n",
      "F1 score tensor(0.0339)\n",
      "epoch 32 : train_loss: 0.7557691602052841 val_loss: 0.02312212036206172 val_f1: tensor(0.0349)\n",
      "Train loss 0.939475417137146 t=0.23237323760986328s\n",
      "112\n",
      "t=0.26011037826538086s\n",
      "F1 score tensor(0.0339)\n",
      "epoch 33 : train_loss: 0.7564962875526563 val_loss: 0.023563472124246452 val_f1: tensor(0.0349)\n",
      "Train loss 0.7153135538101196 t=0.22376227378845215s\n",
      "112\n",
      "t=0.2520298957824707s\n",
      "F1 score tensor(0.0339)\n",
      "epoch 34 : train_loss: 0.7573776242479814 val_loss: 0.06153624791365404 val_f1: tensor(0.0349)\n",
      "Train loss 0.7714895606040955 t=0.22579383850097656s\n",
      "112\n",
      "t=0.26098108291625977s\n",
      "F1 score tensor(0.0339)\n",
      "epoch 35 : train_loss: 0.7580669477450109 val_loss: 0.042503563257364124 val_f1: tensor(0.0349)\n",
      "Train loss 0.7492151856422424 t=0.2265770435333252s\n",
      "112\n",
      "t=0.2565634250640869s\n",
      "F1 score tensor(0.0339)\n",
      "epoch 36 : train_loss: 0.7568312089527602 val_loss: 0.04117078506029569 val_f1: tensor(0.0349)\n",
      "Train loss 0.42411884665489197 t=0.2249915599822998s\n",
      "112\n",
      "t=0.25484180450439453s\n",
      "F1 score tensor(0.0339)\n",
      "epoch 37 : train_loss: 0.7576562134565505 val_loss: 0.06231406560310951 val_f1: tensor(0.0349)\n",
      "Train loss 0.7305490970611572 t=0.23057079315185547s\n",
      "112\n",
      "t=0.2626054286956787s\n",
      "F1 score tensor(0.0339)\n",
      "epoch 38 : train_loss: 0.7595197510930289 val_loss: 0.06075155735015869 val_f1: tensor(0.0349)\n",
      "Train loss 0.6243743300437927 t=0.24636316299438477s\n",
      "112\n",
      "t=0.2736318111419678s\n",
      "F1 score tensor(0.0339)\n",
      "epoch 39 : train_loss: 0.7575042279420701 val_loss: 0.061921729491307184 val_f1: tensor(0.0349)\n",
      "Train loss 0.8270528316497803 t=0.21790146827697754s\n",
      "112\n",
      "t=0.24695444107055664s\n",
      "F1 score tensor(0.0339)\n",
      "epoch 40 : train_loss: 0.7553917399022432 val_loss: 0.04158494105705848 val_f1: tensor(0.0349)\n",
      "Train loss 0.6404448747634888 t=0.21908783912658691s\n",
      "112\n",
      "t=0.24608683586120605s\n",
      "F1 score tensor(0.0339)\n",
      "epoch 41 : train_loss: 0.7564068821679174 val_loss: 0.04093161454567543 val_f1: tensor(0.0349)\n",
      "Train loss 0.8553159236907959 t=0.21535730361938477s\n",
      "112\n",
      "t=0.24275493621826172s\n",
      "F1 score tensor(0.0339)\n",
      "epoch 42 : train_loss: 0.7562427816137803 val_loss: 0.042330530973581165 val_f1: tensor(0.0349)\n",
      "Train loss 0.7286062240600586 t=0.23004817962646484s\n",
      "112\n",
      "t=0.2607533931732178s\n",
      "F1 score tensor(0.0339)\n",
      "epoch 43 : train_loss: 0.7574164603663757 val_loss: 0.042136504099919245 val_f1: tensor(0.0349)\n",
      "Train loss 0.6456856727600098 t=0.2315366268157959s\n",
      "112\n",
      "t=0.2587885856628418s\n",
      "F1 score tensor(0.0339)\n",
      "epoch 44 : train_loss: 0.755526752071043 val_loss: 0.05874344935783973 val_f1: tensor(0.0349)\n",
      "Train loss 0.8421622514724731 t=0.2228085994720459s\n",
      "112\n",
      "t=0.24931788444519043s\n",
      "F1 score tensor(0.0339)\n",
      "epoch 45 : train_loss: 0.7550643173466741 val_loss: 0.10163621719066913 val_f1: tensor(0.0349)\n",
      "Train loss 1.0269397497177124 t=0.22222352027893066s\n",
      "112\n",
      "t=0.25045347213745117s\n",
      "F1 score tensor(0.0339)\n",
      "epoch 46 : train_loss: 0.7562186657327467 val_loss: 0.04204806456199059 val_f1: tensor(0.0349)\n",
      "Train loss 0.7224710583686829 t=0.2202467918395996s\n",
      "112\n",
      "t=0.2500643730163574s\n",
      "F1 score tensor(0.0339)\n",
      "epoch 47 : train_loss: 0.7584475832702839 val_loss: 0.08101586195138785 val_f1: tensor(0.0349)\n",
      "Train loss 0.9386059045791626 t=0.21950864791870117s\n",
      "112\n",
      "t=0.24731183052062988s\n",
      "F1 score tensor(0.0339)\n",
      "epoch 48 : train_loss: 0.7588708113252589 val_loss: 0.06165213768298809 val_f1: tensor(0.0349)\n",
      "Train loss 0.6191120743751526 t=0.23021864891052246s\n",
      "112\n",
      "t=0.2613506317138672s\n",
      "F1 score tensor(0.0339)\n",
      "epoch 49 : train_loss: 0.7571479765187322 val_loss: 0.04443815121283898 val_f1: tensor(0.0349)\n",
      "Train loss 0.6316640377044678 t=0.2244858741760254s\n",
      "112\n",
      "t=0.2524220943450928s\n",
      "F1 score tensor(0.0339)\n",
      "epoch 50 : train_loss: 0.7561489633754291 val_loss: 0.03848169858639057 val_f1: tensor(0.0349)\n"
     ]
    }
   ],
   "source": [
    "epochs=50\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train(trainLoader, model, criterion, optimizer)\n",
    "    val_acc,val_loss = validate(valLoader, model, criterion)\n",
    "    print(\n",
    "        \"epoch {} :\".format(epoch + 1), \"train_loss:\", train_loss, 'val_loss:', val_loss,\"val_f1:\", val_acc\n",
    "    )\n",
    "    # epochs += 1\n",
    "# torch.save(model.state_dict(),\"./model/baseline_test.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.74168557,  0.99619937, -0.84154886],\n",
       "       [-0.74168557,  0.99619937, -0.84154886],\n",
       "       [-0.74168557,  0.99619937, -0.84154886],\n",
       "       ...,\n",
       "       [-0.74168557,  0.99619925, -0.84154874],\n",
       "       [-0.74168557,  0.99619925, -0.84154874],\n",
       "       [-0.74168557,  0.99619925, -0.84154874]], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = predict(testLoader, model)\n",
    "pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.5691e-02, 2.5949e+06, 1.5270e-02, 1.5130e-02, 1.9615e-06, 1.4850e-02,\n",
       "         9.9739e-07, 1.5410e-02, 2.6597e-07, 1.5691e-02, 3.4576e-06]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=torch.tensor([])\n",
    "b=torch.from_numpy(np.array(train_df.iloc[[0]]))\n",
    "torch.cat((a,b),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=torch.tensor([])\n",
    "b=torch.tensor([1])\n",
    "torch.cat((a,b),0).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "seq_length=50\n",
    "f=torch.from_numpy(np.array(train_df.iloc[i+seq_length:i+2*seq_length]))\n",
    "f=torch.unsqueeze(f,0)\n",
    "f.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
