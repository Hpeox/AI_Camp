{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, GroupKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, log_loss, mean_squared_log_error\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "import tqdm, sys, os, gc, argparse, warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../data/'\n",
    "\n",
    "train_files = os.listdir(path+'train')\n",
    "train_df = pd.DataFrame()\n",
    "for filename in tqdm.tqdm(train_files):\n",
    "    tmp = pd.read_csv(path+'train/'+filename)\n",
    "    tmp['file'] = filename\n",
    "    train_df = pd.concat([train_df, tmp], axis=0, ignore_index=True)\n",
    "\n",
    "test_files = os.listdir(path+'test')\n",
    "test_df = pd.DataFrame()\n",
    "for filename in tqdm.tqdm(test_files):\n",
    "    tmp = pd.read_csv(path+'test/'+filename)\n",
    "    tmp['file'] = filename\n",
    "    test_df = pd.concat([test_df, tmp], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 时间相关特征\n",
    "train_df['hour'] = train_df['time'].apply(lambda x:int(x.split(':')[0]))\n",
    "test_df['hour'] = test_df['time'].apply(lambda x:int(x.split(':')[0]))\n",
    "\n",
    "train_df['minute'] = train_df['time'].apply(lambda x:int(x.split(':')[1]))\n",
    "test_df['minute'] = test_df['time'].apply(lambda x:int(x.split(':')[1]))\n",
    "\n",
    "# 入模特征\n",
    "cols = [f for f in test_df.columns if f not in ['uuid','time','file']]\n",
    "\n",
    "# 为了保证时间顺序的一致性，故进行排序\n",
    "train_df = train_df.sort_values(['file','time'])\n",
    "test_df = test_df.sort_values(['file','time'])\n",
    "\n",
    "# 当前时间特征\n",
    "# 围绕买卖价格和买卖量进行构建\n",
    "# 暂时只构建买一卖一和买二卖二相关特征，进行优化时可以加上其余买卖信息\n",
    "train_df['wap1'] = (train_df['n_bid1']*train_df['n_bsize1'] + train_df['n_ask1']*train_df['n_asize1'])/(train_df['n_bsize1'] + train_df['n_asize1'])\n",
    "test_df['wap1'] = (test_df['n_bid1']*test_df['n_bsize1'] + test_df['n_ask1']*test_df['n_asize1'])/(test_df['n_bsize1'] + test_df['n_asize1'])\n",
    "\n",
    "train_df['wap2'] = (train_df['n_bid2']*train_df['n_bsize2'] + train_df['n_ask2']*train_df['n_asize2'])/(train_df['n_bsize2'] + train_df['n_asize2'])\n",
    "test_df['wap2'] = (test_df['n_bid2']*test_df['n_bsize2'] + test_df['n_ask2']*test_df['n_asize2'])/(test_df['n_bsize2'] + test_df['n_asize2'])\n",
    "\n",
    "train_df['wap_balance'] = abs(train_df['wap1'] - train_df['wap2'])\n",
    "train_df['price_spread'] = (train_df['n_ask1'] - train_df['n_bid1']) / ((train_df['n_ask1'] + train_df['n_bid1'])/2)\n",
    "train_df['bid_spread'] = train_df['n_bid1'] - train_df['n_bid2']\n",
    "train_df['ask_spread'] = train_df['n_ask1'] - train_df['n_ask2']\n",
    "train_df['total_volume'] = (train_df['n_asize1'] + train_df['n_asize2']) + (train_df['n_bsize1'] + train_df['n_bsize2'])\n",
    "train_df['volume_imbalance'] = abs((train_df['n_asize1'] + train_df['n_asize2']) - (train_df['n_bsize1'] + train_df['n_bsize2']))\n",
    "\n",
    "test_df['wap_balance'] = abs(test_df['wap1'] - test_df['wap2'])\n",
    "test_df['price_spread'] = (test_df['n_ask1'] - test_df['n_bid1']) / ((test_df['n_ask1'] + test_df['n_bid1'])/2)\n",
    "test_df['bid_spread'] = test_df['n_bid1'] - test_df['n_bid2']\n",
    "test_df['ask_spread'] = test_df['n_ask1'] - test_df['n_ask2']\n",
    "test_df['total_volume'] = (test_df['n_asize1'] + test_df['n_asize2']) + (test_df['n_bsize1'] + test_df['n_bsize2'])\n",
    "test_df['volume_imbalance'] = abs((test_df['n_asize1'] + test_df['n_asize2']) - (test_df['n_bsize1'] + test_df['n_bsize2']))\n",
    "\n",
    "# 历史平移\n",
    "# 获取历史信息\n",
    "for val in ['wap1','wap2','wap_balance','price_spread','bid_spread','ask_spread','total_volume','volume_imbalance']:\n",
    "    for loc in [1,5,10,20,40,60]:\n",
    "        train_df[f'file_{val}_shift{loc}'] = train_df.groupby(['file'])[val].shift(loc)\n",
    "        test_df[f'file_{val}_shift{loc}'] = test_df.groupby(['file'])[val].shift(loc)\n",
    "    \n",
    "# 差分特征\n",
    "# 获取与历史数据的增长关系\n",
    "for val in ['wap1','wap2','wap_balance','price_spread','bid_spread','ask_spread','total_volume','volume_imbalance']:\n",
    "    for loc in [1,5,10,20,40,60]:\n",
    "        train_df[f'file_{val}_diff{loc}'] = train_df.groupby(['file'])[val].diff(loc)\n",
    "        test_df[f'file_{val}_diff{loc}'] = test_df.groupby(['file'])[val].diff(loc)\n",
    "    \n",
    "# 窗口统计\n",
    "# 获取历史信息分布变化信息\n",
    "# 可以尝试更多窗口大小已经统计方式，如min、max、median等\n",
    "for val in ['wap1','wap2','wap_balance','price_spread','bid_spread','ask_spread','total_volume','volume_imbalance']:\n",
    "    train_df[f'file_{val}_win7_mean'] = train_df.groupby(['file'])[val].transform(lambda x: x.rolling(window=7, min_periods=3).mean())\n",
    "    train_df[f'file_{val}_win7_std'] = train_df.groupby(['file'])[val].transform(lambda x: x.rolling(window=7, min_periods=3).std())\n",
    "    \n",
    "    test_df[f'file_{val}_win7_mean'] = test_df.groupby(['file'])[val].transform(lambda x: x.rolling(window=7, min_periods=3).mean())\n",
    "    test_df[f'file_{val}_win7_std'] = test_df.groupby(['file'])[val].transform(lambda x: x.rolling(window=7, min_periods=3).std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_model(clf, train_x, train_y, test_x, clf_name, seed = 2023):\n",
    "    folds = 5\n",
    "    kf = KFold(n_splits=folds, shuffle=True, random_state=seed)\n",
    "    oof = np.zeros([train_x.shape[0], 3])\n",
    "    test_predict = np.zeros([test_x.shape[0], 3])\n",
    "    cv_scores = []\n",
    "    \n",
    "    for i, (train_index, valid_index) in enumerate(kf.split(train_x, train_y)):\n",
    "        print('************************************ {} ************************************'.format(str(i+1)))\n",
    "        trn_x, trn_y, val_x, val_y = train_x.iloc[train_index], train_y[train_index], train_x.iloc[valid_index], train_y[valid_index]\n",
    "        \n",
    "        if clf_name == \"lgb\":\n",
    "            train_matrix = clf.Dataset(trn_x, label=trn_y)\n",
    "            valid_matrix = clf.Dataset(val_x, label=val_y)\n",
    "            params = {\n",
    "                'boosting_type': 'gbdt',\n",
    "                'objective': 'multiclass',\n",
    "                'num_class':3,\n",
    "                'min_child_weight': 6,\n",
    "                'num_leaves': 2 ** 6,\n",
    "                'lambda_l2': 10,\n",
    "                'feature_fraction': 0.8,\n",
    "                'bagging_fraction': 0.8,\n",
    "                'bagging_freq': 4,\n",
    "                'learning_rate': 0.2,\n",
    "                'seed': 2023,\n",
    "                'nthread' : 16,\n",
    "                'verbose' : -1,\n",
    "                # \"device\" : \"gpu\"\n",
    "            }\n",
    "            model = clf.train(params, train_matrix, 300, valid_sets=[train_matrix, valid_matrix],\n",
    "                              categorical_feature=[])\n",
    "            val_pred = model.predict(val_x, num_iteration=model.best_iteration)\n",
    "            test_pred = model.predict(test_x, num_iteration=model.best_iteration)\n",
    "        \n",
    "        if clf_name == \"xgb\":\n",
    "            xgb_params = {\n",
    "              'booster': 'gbtree', \n",
    "              'objective': 'multi:softprob',\n",
    "              'num_class':3,\n",
    "              'max_depth': 8,\n",
    "              'lambda': 10,\n",
    "              'subsample': 0.7,\n",
    "              'colsample_bytree': 0.7,\n",
    "              'colsample_bylevel': 0.7,\n",
    "              'eta': 0.15,\n",
    "              'tree_method': 'gpu_hist',\n",
    "              'seed': 520,\n",
    "              'nthread': 16\n",
    "              }\n",
    "            train_matrix = clf.DMatrix(trn_x , label=trn_y)\n",
    "            valid_matrix = clf.DMatrix(val_x , label=val_y)\n",
    "            test_matrix = clf.DMatrix(test_x)\n",
    "            \n",
    "            watchlist = [(train_matrix, 'train'),(valid_matrix, 'eval')]\n",
    "            \n",
    "            model = clf.train(xgb_params, train_matrix, num_boost_round=300, evals=watchlist)\n",
    "            val_pred  = model.predict(valid_matrix)\n",
    "            test_pred = model.predict(test_matrix)\n",
    "            \n",
    "        if clf_name == \"cat\":\n",
    "            params = {'learning_rate': 0.15, 'depth': 5, 'bootstrap_type':'Bernoulli','random_seed':2023,\n",
    "                      'od_type': 'Iter', 'od_wait': 100, 'random_seed': 11, 'allow_writing_files': False,\n",
    "                      'loss_function': 'MultiClass'}\n",
    "            \n",
    "            model = clf(iterations=400, task_type=\"GPU\",**params)\n",
    "            model.fit(trn_x, trn_y, eval_set=(val_x, val_y),\n",
    "                      metric_period=100,\n",
    "                      use_best_model=True, \n",
    "                      cat_features=[],\n",
    "                      verbose=1)\n",
    "            \n",
    "            val_pred  = model.predict_proba(val_x)\n",
    "            test_pred = model.predict_proba(test_x)\n",
    "        \n",
    "        oof[valid_index] = val_pred\n",
    "        test_predict += test_pred / kf.n_splits\n",
    "        \n",
    "        F1_score = f1_score(val_y, np.argmax(val_pred, axis=1), average='macro')\n",
    "        cv_scores.append(F1_score)\n",
    "        print(cv_scores)\n",
    "        \n",
    "    return oof, test_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in ['label_5','label_10','label_20','label_40','label_60']:\n",
    "    print(f'=================== {label} ===================')\n",
    "    # 选择lightgbm模型\n",
    "    lgb_oof, lgb_test = cv_model(lgb, train_df[cols], train_df[label], test_df[cols], 'lgb')\n",
    "    # 选择xgboost模型\n",
    "    xgb_oof, xgb_test = cv_model(xgb, train_df[cols], train_df[label], test_df[cols], 'xgb')\n",
    "    # 选择catboost模型\n",
    "    cat_oof, cat_test = cv_model(CatBoostClassifier, train_df[cols], train_df[label], test_df[cols], 'cat')\n",
    "\n",
    "    # 进行取平均融合\n",
    "    final_test = (lgb_test + xgb_test + cat_test) / 3\n",
    "    # final_test = cat_test\n",
    "\n",
    "    test_df[label] = np.argmax(final_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检查并删除'submit'文件夹\n",
    "if os.path.exists('./submit'):\n",
    "    shutil.rmtree('./submit')\n",
    "    print(\"Removed the 'submit' directory.\")\n",
    "\n",
    "# # 检查并删除'submit.zip'文件\n",
    "if os.path.isfile('./advanced/submit.zip'):\n",
    "    os.remove('./advanced/submit.zip')\n",
    "    print(\"Removed the 'submit.zip' file.\")\n",
    "\n",
    "# 指定输出文件夹路径\n",
    "output_dir = './submit'\n",
    "\n",
    "# 如果文件夹不存在则创建\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# 首先按照'file'字段对 dataframe 进行分组\n",
    "grouped = test_df.groupby('file')\n",
    "\n",
    "# 对于每一个group进行处理\n",
    "for file_name, group in grouped:\n",
    "    # 选择你所需要的列\n",
    "    selected_cols = group[['uuid', 'label_5', 'label_10', 'label_20', 'label_40', 'label_60']]\n",
    "    \n",
    "    # 将其保存为csv文件，file_name作为文件名\n",
    "    selected_cols.to_csv(os.path.join(output_dir, f'{file_name}'), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
