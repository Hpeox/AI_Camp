{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m torch\u001b[39m.\u001b[39mbackends\u001b[39m.\u001b[39mcudnn\u001b[39m.\u001b[39mdeterministic \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m     13\u001b[0m torch\u001b[39m.\u001b[39mbackends\u001b[39m.\u001b[39mcudnn\u001b[39m.\u001b[39mbenchmark \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorchmetrics\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mmodels\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[39m# import torchvision.transforms as transforms\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[39m# import torchvision.datasets as datasets\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\H.peox\\.conda\\envs\\Pytorch2\\lib\\site-packages\\torchmetrics\\__init__.py:14\u001b[0m\n\u001b[0;32m     11\u001b[0m _PACKAGE_ROOT \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mdirname(\u001b[39m__file__\u001b[39m)\n\u001b[0;32m     12\u001b[0m _PROJECT_ROOT \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mdirname(_PACKAGE_ROOT)\n\u001b[1;32m---> 14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m functional  \u001b[39m# noqa: E402\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchmetrics\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39maggregation\u001b[39;00m \u001b[39mimport\u001b[39;00m CatMetric, MaxMetric, MeanMetric, MinMetric, SumMetric  \u001b[39m# noqa: E402\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchmetrics\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39maudio\u001b[39;00m \u001b[39mimport\u001b[39;00m (  \u001b[39m# noqa: E402\u001b[39;00m\n\u001b[0;32m     17\u001b[0m     PermutationInvariantTraining,\n\u001b[0;32m     18\u001b[0m     ScaleInvariantSignalDistortionRatio,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     21\u001b[0m     SignalNoiseRatio,\n\u001b[0;32m     22\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\H.peox\\.conda\\envs\\Pytorch2\\lib\\site-packages\\torchmetrics\\functional\\__init__.py:14\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Copyright The PyTorch Lightning team.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[39m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[39m# limitations under the License.\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchmetrics\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunctional\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39maudio\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpit\u001b[39;00m \u001b[39mimport\u001b[39;00m permutation_invariant_training, pit_permutate\n\u001b[0;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchmetrics\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunctional\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39maudio\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msdr\u001b[39;00m \u001b[39mimport\u001b[39;00m scale_invariant_signal_distortion_ratio, signal_distortion_ratio\n\u001b[0;32m     16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchmetrics\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunctional\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39maudio\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msnr\u001b[39;00m \u001b[39mimport\u001b[39;00m scale_invariant_signal_noise_ratio, signal_noise_ratio\n",
      "File \u001b[1;32mc:\\Users\\H.peox\\.conda\\envs\\Pytorch2\\lib\\site-packages\\torchmetrics\\functional\\audio\\__init__.py:14\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Copyright The PyTorch Lightning team.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[39m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[39m# limitations under the License.\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchmetrics\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunctional\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39maudio\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpit\u001b[39;00m \u001b[39mimport\u001b[39;00m permutation_invariant_training, pit_permutate  \u001b[39m# noqa: F401\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchmetrics\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunctional\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39maudio\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msdr\u001b[39;00m \u001b[39mimport\u001b[39;00m (  \u001b[39m# noqa: F401\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     scale_invariant_signal_distortion_ratio,\n\u001b[0;32m     17\u001b[0m     signal_distortion_ratio,\n\u001b[0;32m     18\u001b[0m )\n\u001b[0;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchmetrics\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunctional\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39maudio\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msnr\u001b[39;00m \u001b[39mimport\u001b[39;00m scale_invariant_signal_noise_ratio, signal_noise_ratio  \u001b[39m# noqa: F401\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\H.peox\\.conda\\envs\\Pytorch2\\lib\\site-packages\\torchmetrics\\functional\\audio\\pit.py:22\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m \u001b[39mimport\u001b[39;00m Tensor\n\u001b[0;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping_extensions\u001b[39;00m \u001b[39mimport\u001b[39;00m Literal\n\u001b[1;32m---> 22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchmetrics\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutilities\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mimports\u001b[39;00m \u001b[39mimport\u001b[39;00m _SCIPY_AVAILABLE\n\u001b[0;32m     24\u001b[0m \u001b[39m# _ps_dict: cache of permutations\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[39m# it's necessary to cache it, otherwise it will consume a large amount of time\u001b[39;00m\n\u001b[0;32m     26\u001b[0m _ps_dict: \u001b[39mdict\u001b[39m \u001b[39m=\u001b[39m {}  \u001b[39m# _ps_dict[str(spk_num)+str(device)] = permutations\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\H.peox\\.conda\\envs\\Pytorch2\\lib\\site-packages\\torchmetrics\\utilities\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchmetrics\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutilities\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mchecks\u001b[39;00m \u001b[39mimport\u001b[39;00m check_forward_full_state_property  \u001b[39m# noqa: F401\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchmetrics\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutilities\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m apply_to_collection  \u001b[39m# noqa: F401\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchmetrics\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutilities\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdistributed\u001b[39;00m \u001b[39mimport\u001b[39;00m class_reduce, reduce  \u001b[39m# noqa: F401\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\H.peox\\.conda\\envs\\Pytorch2\\lib\\site-packages\\torchmetrics\\utilities\\checks.py:25\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m \u001b[39mimport\u001b[39;00m Tensor\n\u001b[1;32m---> 25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchmetrics\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutilities\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m select_topk, to_onehot\n\u001b[0;32m     26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchmetrics\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutilities\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39menums\u001b[39;00m \u001b[39mimport\u001b[39;00m DataType\n\u001b[0;32m     28\u001b[0m _DOCTEST_DOWNLOAD_TIMEOUT \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39menviron\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mDOCTEST_DOWNLOAD_TIMEOUT\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m120\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\H.peox\\.conda\\envs\\Pytorch2\\lib\\site-packages\\torchmetrics\\utilities\\data.py:19\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m \u001b[39mimport\u001b[39;00m Tensor\n\u001b[1;32m---> 19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchmetrics\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutilities\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mimports\u001b[39;00m \u001b[39mimport\u001b[39;00m _TORCH_GREATER_EQUAL_1_12, _XLA_AVAILABLE\n\u001b[0;32m     21\u001b[0m METRIC_EPS \u001b[39m=\u001b[39m \u001b[39m1e-6\u001b[39m\n\u001b[0;32m     24\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdim_zero_cat\u001b[39m(x: Union[Tensor, List[Tensor]]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n",
      "File \u001b[1;32mc:\\Users\\H.peox\\.conda\\envs\\Pytorch2\\lib\\site-packages\\torchmetrics\\utilities\\imports.py:112\u001b[0m\n\u001b[0;32m    110\u001b[0m _PYCOCOTOOLS_AVAILABLE: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m _package_available(\u001b[39m\"\u001b[39m\u001b[39mpycocotools\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    111\u001b[0m _TORCHVISION_AVAILABLE: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m _package_available(\u001b[39m\"\u001b[39m\u001b[39mtorchvision\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 112\u001b[0m _TORCHVISION_GREATER_EQUAL_0_8: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m _compare_version(\u001b[39m\"\u001b[39;49m\u001b[39mtorchvision\u001b[39;49m\u001b[39m\"\u001b[39;49m, operator\u001b[39m.\u001b[39;49mge, \u001b[39m\"\u001b[39;49m\u001b[39m0.8.0\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m    113\u001b[0m _TQDM_AVAILABLE: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m _package_available(\u001b[39m\"\u001b[39m\u001b[39mtqdm\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    114\u001b[0m _TRANSFORMERS_AVAILABLE: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m _package_available(\u001b[39m\"\u001b[39m\u001b[39mtransformers\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\H.peox\\.conda\\envs\\Pytorch2\\lib\\site-packages\\torchmetrics\\utilities\\imports.py:78\u001b[0m, in \u001b[0;36m_compare_version\u001b[1;34m(package, op, version)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[39m@lru_cache\u001b[39m()\n\u001b[0;32m     70\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_compare_version\u001b[39m(package: \u001b[39mstr\u001b[39m, op: Callable, version: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Optional[\u001b[39mbool\u001b[39m]:\n\u001b[0;32m     71\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Compare package version with some requirements.\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \n\u001b[0;32m     73\u001b[0m \u001b[39m    >>> import operator\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[39m    >>> _compare_version(\"any_module\", operator.ge, \"0.0\")  # is None\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 78\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _module_available(package):\n\u001b[0;32m     79\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     80\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\H.peox\\.conda\\envs\\Pytorch2\\lib\\site-packages\\torchmetrics\\utilities\\imports.py:59\u001b[0m, in \u001b[0;36m_module_available\u001b[1;34m(module_path)\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 59\u001b[0m     module \u001b[39m=\u001b[39m import_module(module_names[\u001b[39m0\u001b[39;49m])\n\u001b[0;32m     60\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n\u001b[0;32m     61\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\H.peox\\.conda\\envs\\Pytorch2\\lib\\importlib\\__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    124\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m    125\u001b[0m         level \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m--> 126\u001b[0m \u001b[39mreturn\u001b[39;00m _bootstrap\u001b[39m.\u001b[39;49m_gcd_import(name[level:], package, level)\n",
      "File \u001b[1;32mc:\\Users\\H.peox\\.conda\\envs\\Pytorch2\\lib\\site-packages\\torchvision\\__init__.py:6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmodulefinder\u001b[39;00m \u001b[39mimport\u001b[39;00m Module\n\u001b[0;32m      5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m \u001b[39mimport\u001b[39;00m datasets, io, models, ops, transforms, utils\n\u001b[0;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mextension\u001b[39;00m \u001b[39mimport\u001b[39;00m _HAS_OPS\n\u001b[0;32m     10\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\H.peox\\.conda\\envs\\Pytorch2\\lib\\site-packages\\torchvision\\datasets\\__init__.py:24\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39meurosat\u001b[39;00m \u001b[39mimport\u001b[39;00m EuroSAT\n\u001b[0;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mfakedata\u001b[39;00m \u001b[39mimport\u001b[39;00m FakeData\n\u001b[1;32m---> 24\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mfer2013\u001b[39;00m \u001b[39mimport\u001b[39;00m FER2013\n\u001b[0;32m     25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mfgvc_aircraft\u001b[39;00m \u001b[39mimport\u001b[39;00m FGVCAircraft\n\u001b[0;32m     26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mflickr\u001b[39;00m \u001b[39mimport\u001b[39;00m Flickr30k, Flickr8k\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:879\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:975\u001b[0m, in \u001b[0;36mget_code\u001b[1;34m(self, fullname)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1074\u001b[0m, in \u001b[0;36mget_data\u001b[1;34m(self, path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os, sys, glob, argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# from tqdm import tqdm\n",
    "\n",
    "import cv2, time\n",
    "# from PIL import Image\n",
    "# from sklearn.model_selection import train_test_split, StratifiedKFold, KFold\n",
    "\n",
    "import torch\n",
    "torch.manual_seed(0)\n",
    "torch.backends.cudnn.deterministic = False\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "import torchmetrics\n",
    "import torchvision.models as models\n",
    "# import torchvision.transforms as transforms\n",
    "# import torchvision.datasets as datasets\n",
    "import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# import torch.optim as optim\n",
    "# from torch.autograd import Variable\n",
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "\n",
    "# Check if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 3070 Ti Laptop GPU'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取数据集\n",
    "train_path = glob.glob(\"./data/train/*\")\n",
    "test_path = glob.glob(\"./data/test/*\")\n",
    "\n",
    "train_path.sort()\n",
    "test_path.sort()\n",
    "\n",
    "train_df = pd.read_csv(\"data/train.csv\")\n",
    "train_df = train_df.sort_values(by=\"name\")\n",
    "train_label = train_df[\"label\"].values\n",
    "\n",
    "# 自定义数据集\n",
    "# 带有图片缓存的逻辑\n",
    "DATA_CACHE = {}\n",
    "\n",
    "\n",
    "class XunFeiDataset(Dataset):\n",
    "    def __init__(self, img_path, img_label, transform=None):\n",
    "        self.img_path = img_path\n",
    "        self.img_label = img_label\n",
    "        if transform is not None:\n",
    "            self.transform = transform\n",
    "        else:\n",
    "            self.transform = None\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.img_path[index] in DATA_CACHE:\n",
    "            img = DATA_CACHE[self.img_path[index]]\n",
    "        else:\n",
    "            img = cv2.imread(self.img_path[index])\n",
    "            DATA_CACHE[self.img_path[index]] = img\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(image=img)[\"image\"]\n",
    "        img = img.transpose([2, 0, 1])\n",
    "        return img, torch.from_numpy(np.array(self.img_label[index]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_path)\n",
    "\n",
    "\n",
    "import albumentations as A\n",
    "\n",
    "dataset = XunFeiDataset(\n",
    "    train_path,\n",
    "    train_label,\n",
    "    A.Compose(\n",
    "        [\n",
    "            A.RandomRotate90(),\n",
    "            A.Resize(256, 256),\n",
    "            A.RandomCrop(224, 224),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.RandomBrightnessContrast(p=0.5),\n",
    "            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(\n",
    "    dataset=dataset, lengths=[0.95, 0.05], generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "# 训练集\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=32, shuffle=True, num_workers=0, pin_memory=False\n",
    ")\n",
    "\n",
    "# 验证集\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=32, shuffle=False, num_workers=0, pin_memory=False\n",
    ")\n",
    "\n",
    "# 测试集\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    XunFeiDataset(\n",
    "        test_path,\n",
    "        [0] * len(test_path),\n",
    "        A.Compose(\n",
    "            [\n",
    "                A.Resize(256, 256),\n",
    "                A.RandomCrop(224, 224),\n",
    "                A.HorizontalFlip(p=0.5),\n",
    "                # A.RandomBrightnessContrast(p=0.5),\n",
    "                A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "            ]\n",
    "        ),\n",
    "    ),\n",
    "    batch_size=50,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([17,  5, 15,  1,  0,  5,  0, 11, 17,  3], dtype=int64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XunFeiNet50(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(XunFeiNet50, self).__init__()\n",
    "        model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "        model.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        model.fc = nn.Linear(2048, 25)\n",
    "        self.resnet = model\n",
    "    def forward(self, img):\n",
    "        out = self.resnet(img)\n",
    "        return out\n",
    "        \n",
    "class XunFeiNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(XunFeiNet, self).__init__()\n",
    "        model = models.resnet101(weights=models.ResNet101_Weights.DEFAULT)\n",
    "        model.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        model.fc = nn.Linear(2048, 25)\n",
    "        self.resnet = model\n",
    "    def forward(self, img):\n",
    "        out = self.resnet(img)\n",
    "        return out\n",
    "class XunFeiNet152(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(XunFeiNet152, self).__init__()\n",
    "        model = models.resnet152(weights=models.ResNet152_Weights.DEFAULT)\n",
    "        model.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        model.fc = nn.Linear(2048, 25)\n",
    "        self.resnet = model\n",
    "    def forward(self, img):\n",
    "        out = self.resnet(img)\n",
    "        return out\n",
    "model50 = XunFeiNet50().to(device)\n",
    "model101= XunFeiNet().to(device)\n",
    "model152= XunFeiNet152().to(device)\n",
    "# model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型训练\n",
    "def train(train_loader, model, criterion, optimizer):\n",
    "    start = time.time()\n",
    "    start_batch = [start, 0]\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    preds = torch.tensor([])\n",
    "    target_all = torch.tensor([])\n",
    "    for i, (input, target) in enumerate(train_loader):\n",
    "        input = input.to(device)\n",
    "        target = target.to(device)\n",
    "        output = model(input)\n",
    "        loss = criterion(output, target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i + 1) % 100 == 0:\n",
    "            start_batch[((i + 1) // 100) % 2] = time.time()\n",
    "            print(\n",
    "                \"Train loss\",\n",
    "                loss.item(),\n",
    "                \"t={}s\".format(\n",
    "                    start_batch[((i + 1) // 100) % 2]\n",
    "                    - start_batch[(1 + (i + 1) // 100) % 2]\n",
    "                ),\n",
    "            )\n",
    "\n",
    "        preds = torch.cat((preds, output.cpu().argmax(1)))\n",
    "        target_all = torch.cat((target_all, target.cpu()))\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    val_acc = torchmetrics.functional.classification.multiclass_f1_score(\n",
    "        preds, target_all, num_classes=25, average=\"macro\"\n",
    "    )\n",
    "    print(\"t={}s\".format(time.time() - start))\n",
    "    print(\"F1 score\", val_acc)\n",
    "    return train_loss / len(train_loader)\n",
    "\n",
    "\n",
    "# 模型验证\n",
    "def validate(val_loader, model, criterion):\n",
    "    model.eval()\n",
    "    val_acc = 0.0\n",
    "    preds = torch.tensor([])\n",
    "    target_all = torch.tensor([])\n",
    "    val_loss=0.0\n",
    "    with torch.no_grad():\n",
    "        for i, (input, target) in enumerate(val_loader):\n",
    "            input = input.to(device)\n",
    "            target = target.to(device)\n",
    "            output = model(input)\n",
    "            loss = criterion(output, target)\n",
    "            # val_acc += (output.argmax(1) == target).sum().item()\n",
    "            preds = torch.cat((preds, output.cpu().argmax(1)))\n",
    "            target_all = torch.cat((target_all, target.cpu()))\n",
    "\n",
    "        val_acc = torchmetrics.functional.classification.multiclass_f1_score(\n",
    "            preds, target_all, num_classes=25, average=\"macro\"\n",
    "        )\n",
    "        val_loss += loss.item()\n",
    "    # return val_acc / len(val_loader.dataset)\n",
    "    return val_acc,val_loss/len(val_loader)\n",
    "\n",
    "\n",
    "# 模型预测\n",
    "def predict(test_loader, model):\n",
    "    model.eval()\n",
    "\n",
    "    test_pred = []\n",
    "    with torch.no_grad():\n",
    "        for i, (input, target) in enumerate(test_loader):\n",
    "            input = input.to(device)\n",
    "            output = model(input)\n",
    "            test_pred.append(output.data.cpu().numpy())\n",
    "\n",
    "    return np.vstack(test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for best model\n",
    "model=model152\n",
    "optimizer = torch.optim.AdamW(model.parameters(), 0.0001)\n",
    "# model=model101\n",
    "epochs = 0\n",
    "last_acc = 0\n",
    "val_acc = 0.001\n",
    "while last_acc < val_acc:\n",
    "    last_acc = val_acc\n",
    "    train_loss = train(train_loader, model, criterion, optimizer)\n",
    "    val_acc,val_loss = validate(val_loader, model, criterion)\n",
    "    # train_acc = validate(train_loader, model)\n",
    "    print(\n",
    "        \"epoch {} :\".format(epochs + 1), \"train_loss:\", train_loss, 'val_loss:', val_loss,\"val_f1:\", val_acc\n",
    "    )\n",
    "    epochs += 1\n",
    "    if val_acc>last_acc:\n",
    "        torch.save(model.state_dict(),\"./model/baseline_152.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for best model\n",
    "model=model101\n",
    "optimizer = torch.optim.AdamW(model.parameters(), 0.0001)\n",
    "# model=model101\n",
    "epochs = 0\n",
    "last_acc = 0\n",
    "val_acc = 0.001\n",
    "while last_acc < val_acc:\n",
    "    last_acc = val_acc\n",
    "    train_loss = train(train_loader, model, criterion, optimizer)\n",
    "    val_acc,val_loss = validate(val_loader, model, criterion)\n",
    "    # train_acc = validate(train_loader, model)\n",
    "    print(\n",
    "        \"epoch {} :\".format(epochs + 1), \"train_loss:\", train_loss, 'val_loss:', val_loss,\"val_f1:\", val_acc\n",
    "    )\n",
    "    epochs += 1\n",
    "    if val_acc>last_acc:\n",
    "        torch.save(model.state_dict(),\"./model/baseline_101.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for best model\n",
    "model=model50\n",
    "optimizer = torch.optim.AdamW(model.parameters(), 0.0001)\n",
    "# model=model101\n",
    "epochs = 0\n",
    "last_acc = 0\n",
    "val_acc = 0.001\n",
    "while last_acc < val_acc:\n",
    "    last_acc = val_acc\n",
    "    train_loss = train(train_loader, model, criterion, optimizer)\n",
    "    val_acc,val_loss = validate(val_loader, model, criterion)\n",
    "    # train_acc = validate(train_loader, model)\n",
    "    print(\n",
    "        \"epoch {} :\".format(epochs + 1), \"train_loss:\", train_loss, 'val_loss:', val_loss,\"val_f1:\", val_acc\n",
    "    )\n",
    "    epochs += 1\n",
    "    if val_acc>last_acc:\n",
    "        torch.save(model.state_dict(),\"./model/baseline_50.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XunFeiNet34(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(XunFeiNet34, self).__init__()\n",
    "        model = models.resnet34(weights=models.ResNet34_Weights.DEFAULT)\n",
    "        model.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        model.fc = nn.Linear(512, 25)\n",
    "        self.resnet = model\n",
    "    def forward(self, img):\n",
    "        out = self.resnet(img)\n",
    "        return out\n",
    "model34 = XunFeiNet50().to(device)\n",
    "\n",
    "# for best model\n",
    "model=model34\n",
    "optimizer = torch.optim.AdamW(model.parameters(), 0.0001)\n",
    "# model=model101\n",
    "epochs = 0\n",
    "last_acc = 0\n",
    "val_acc = 0.001\n",
    "while last_acc < val_acc:\n",
    "    last_acc = val_acc\n",
    "    train_loss = train(train_loader, model, criterion, optimizer)\n",
    "    val_acc,val_loss = validate(val_loader, model, criterion)\n",
    "    # train_acc = validate(train_loader, model)\n",
    "    print(\n",
    "        \"epoch {} :\".format(epochs + 1), \"train_loss:\", train_loss, 'val_loss:', val_loss,\"val_f1:\", val_acc\n",
    "    )\n",
    "    epochs += 1\n",
    "    if val_acc>last_acc:\n",
    "        torch.save(model.state_dict(),\"./model/baseline_34.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对测试集多次预测\n",
    "pred = None\n",
    "model=model101\n",
    "model.load_state_dict(torch.load(\"./model/baseline_101.pth\"))\n",
    "for _ in range(3):\n",
    "    if pred is None:\n",
    "        pred = predict(test_loader, model)\n",
    "    else:\n",
    "        pred += predict(test_loader, model)\n",
    "    print(_+1)\n",
    "model=model152\n",
    "model.load_state_dict(torch.load(\"./model/baseline_152.pth\"))\n",
    "for _ in range(3):\n",
    "    if pred is None:\n",
    "        pred = predict(test_loader, model)\n",
    "    else:\n",
    "        pred += 0.9*predict(test_loader, model)\n",
    "    print(_+1)\n",
    "model=model50\n",
    "model.load_state_dict(torch.load(\"./model/baseline_50.pth\"))\n",
    "for _ in range(3):\n",
    "    if pred is None:\n",
    "        pred = predict(test_loader, model)\n",
    "    else:\n",
    "        pred += predict(test_loader, model)\n",
    "    print(_+1)\n",
    "model=model34\n",
    "model.load_state_dict(torch.load(\"./model/baseline_34.pth\"))\n",
    "for _ in range(3):\n",
    "    if pred is None:\n",
    "        pred = predict(test_loader, model)\n",
    "    else:\n",
    "        pred += 0.9*predict(test_loader, model)\n",
    "    print(_+1)\n",
    "submit = pd.DataFrame(\n",
    "    {\n",
    "        'name': [x.split('/')[-1] for x in test_path],\n",
    "        'label': pred.argmax(1)\n",
    "})\n",
    "\n",
    "# 生成提交结果\n",
    "submit = submit.sort_values(by='name')\n",
    "submit.to_csv('submit6.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "train_loss = 0.0\n",
    "model = model.to(device)\n",
    "for i, (input, target) in enumerate(train_loader):\n",
    "    input = input.to(device)\n",
    "    target = target.to(device)\n",
    "    output = model(input)\n",
    "    loss = criterion(output, target)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=4\n",
    "model=model101\n",
    "optimizer = torch.optim.AdamW(model.parameters(), 0.0001)\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train(train_loader, model, criterion, optimizer)\n",
    "    val_acc,val_loss = validate(val_loader, model, criterion)\n",
    "    print(\n",
    "        \"epoch {} :\".format(epoch + 1), \"train_loss:\", train_loss, 'val_loss:', val_loss,\"val_f1:\", val_acc\n",
    "    )\n",
    "    epochs += 1\n",
    "torch.save(model.state_dict(),\"./model/baseline_test.pth\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=3\n",
    "model101= XunFeiNet().to(device)\n",
    "model=model101\n",
    "optimizer = torch.optim.AdamW(model.parameters(), 0.0001)\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train(train_loader, model, criterion, optimizer)\n",
    "    val_acc,val_loss = validate(val_loader, model, criterion)\n",
    "    print(\n",
    "        \"epoch {} :\".format(epoch + 1), \"train_loss:\", train_loss, 'val_loss:', val_loss,\"val_f1:\", val_acc\n",
    "    )\n",
    "    epochs += 1\n",
    "torch.save(model.state_dict(),\"./model/baseline_101.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = None\n",
    "model=model101\n",
    "model.load_state_dict(torch.load(\"./model/baseline_test.pth\"))\n",
    "for _ in range(3):\n",
    "    if pred is None:\n",
    "        pred = predict(test_loader, model)\n",
    "    else:\n",
    "        pred += predict(test_loader, model)\n",
    "    print(_+1)\n",
    "model.load_state_dict(torch.load(\"./model/baseline_101.pth\"))\n",
    "for _ in range(3):\n",
    "    if pred is None:\n",
    "        pred = predict(test_loader, model)\n",
    "    else:\n",
    "        pred += predict(test_loader, model)\n",
    "    print(_+1)\n",
    "model=model50\n",
    "model.load_state_dict(torch.load(\"./model/baseline_50.pth\"))\n",
    "for _ in range(3):\n",
    "    if pred is None:\n",
    "        pred = predict(test_loader, model)\n",
    "    else:\n",
    "        pred += predict(test_loader, model)\n",
    "    print(_+1)\n",
    "submit = pd.DataFrame(\n",
    "    {\n",
    "        'name': [x.split('/')[-1] for x in test_path],\n",
    "        'label': pred.argmax(1)\n",
    "})\n",
    "\n",
    "# 生成提交结果\n",
    "submit = submit.sort_values(by='name')\n",
    "submit.to_csv('submit5.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet101(weights=models.ResNet101_Weights.DEFAULT)\n",
    "model.fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_acc = validate(val_loader, model)\n",
    "val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=XunFeiDataset(train_path[:-1000], train_label[:-1000],\n",
    "            A.Compose([\n",
    "            # A.RandomRotate90(),\n",
    "            A.Resize(256, 256),\n",
    "            A.RandomCrop(224, 224),\n",
    "            # A.HorizontalFlip(p=0.5),\n",
    "            # A.RandomBrightnessContrast(p=0.5),\n",
    "            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "        ])\n",
    "        )\n",
    "train_dataset, test_dataset=torch.utils.data.random_split(dataset=dataset,lengths=[0.9,0.1],generator=torch.Generator().manual_seed(42))\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"./model/baseline_test.pth\"))\n",
    "val_acc,val_loss = validate(val_loader, model, criterion)\n",
    "print(val_loss)\n",
    "len(val_loader.dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
