{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, glob, argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import cv2, time\n",
    "# from PIL import Image\n",
    "# from sklearn.model_selection import train_test_split, StratifiedKFold, KFold\n",
    "\n",
    "import torch\n",
    "torch.manual_seed(0)\n",
    "torch.backends.cudnn.deterministic = False\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "import torchmetrics\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "\n",
    "# Check if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取数据集\n",
    "train_path = glob.glob('./data/train/*')\n",
    "test_path = glob.glob('./data/test/*')\n",
    "\n",
    "train_path.sort()\n",
    "test_path.sort()\n",
    "\n",
    "train_df = pd.read_csv('data/train.csv')\n",
    "train_df = train_df.sort_values(by='name')\n",
    "train_label = train_df['label'].values\n",
    "\n",
    "# 自定义数据集\n",
    "# 带有图片缓存的逻辑\n",
    "DATA_CACHE = {}\n",
    "class XunFeiDataset(Dataset):\n",
    "    def __init__(self, img_path, img_label, transform=None):\n",
    "        self.img_path = img_path\n",
    "        self.img_label = img_label\n",
    "        if transform is not None:\n",
    "            self.transform = transform\n",
    "        else:\n",
    "            self.transform = None\n",
    "    def __getitem__(self, index):\n",
    "        if self.img_path[index] in DATA_CACHE:\n",
    "            img = DATA_CACHE[self.img_path[index]]\n",
    "        else:\n",
    "            img = cv2.imread(self.img_path[index])\n",
    "            DATA_CACHE[self.img_path[index]] = img\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(image = img)['image']\n",
    "        img = img.transpose([2,0,1])\n",
    "        return img, torch.from_numpy(np.array(self.img_label[index]))\n",
    "    def __len__(self):\n",
    "        return len(self.img_path)\n",
    "        \n",
    "import albumentations as A\n",
    "# 训练集\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    XunFeiDataset(train_path[:-1000], train_label[:-1000],\n",
    "            A.Compose([\n",
    "            # A.RandomRotate90(),\n",
    "            A.Resize(256, 256),\n",
    "            A.RandomCrop(224, 224),\n",
    "            # A.HorizontalFlip(p=0.5),\n",
    "            # A.RandomBrightnessContrast(p=0.5),\n",
    "            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "        ])\n",
    "    ), batch_size=30, shuffle=True, num_workers=0, pin_memory=False\n",
    ")\n",
    "\n",
    "# 验证集\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    XunFeiDataset(train_path[-1000:], train_label[-1000:],\n",
    "            A.Compose([\n",
    "            A.Resize(256, 256),\n",
    "            A.RandomCrop(224, 224),\n",
    "            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "        ])\n",
    "    ), batch_size=50, shuffle=False, num_workers=0, pin_memory=False\n",
    ")\n",
    "\n",
    "# 测试集\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    XunFeiDataset(test_path, [0] * len(test_path),\n",
    "            A.Compose([\n",
    "            A.Resize(256, 256),\n",
    "            A.RandomCrop(224, 224),\n",
    "            # A.HorizontalFlip(p=0.5),\n",
    "            # A.RandomBrightnessContrast(p=0.5),\n",
    "            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "        ])\n",
    "    ), batch_size=50, shuffle=False, num_workers=0, pin_memory=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XunFeiNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(XunFeiNet, self).__init__()\n",
    "        model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "        model.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        model.fc = nn.Linear(2048, 25)\n",
    "        self.resnet = model\n",
    "    def forward(self, img):\n",
    "        out = self.resnet(img)\n",
    "        return out\n",
    "        \n",
    "model = XunFeiNet()\n",
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型训练\n",
    "def train(train_loader, model, criterion, optimizer):\n",
    "    start = time.time()\n",
    "    start_batch = [start, 0]\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    preds = torch.tensor([])\n",
    "    target_all = torch.tensor([])\n",
    "    for i, (input, target) in enumerate(train_loader):\n",
    "        input = input.to(device)\n",
    "        target = target.to(device)\n",
    "        output = model(input)\n",
    "        loss = criterion(output, target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i + 1) % 100 == 0:\n",
    "            start_batch[((i + 1) // 100) % 2] = time.time()\n",
    "            print(\n",
    "                \"Train loss\",\n",
    "                loss.item(),\n",
    "                \"t={}s\".format(\n",
    "                    start_batch[((i + 1) // 100) % 2]\n",
    "                    - start_batch[(1 + (i + 1) // 100) % 2]\n",
    "                ),\n",
    "            )\n",
    "\n",
    "        preds = torch.cat((preds, output.cpu().argmax(1)))\n",
    "        target_all = torch.cat((target_all, target.cpu()))\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    val_acc = torchmetrics.functional.classification.multiclass_f1_score(\n",
    "        preds, target_all, num_classes=25, average=\"macro\"\n",
    "    )\n",
    "    print(\"t={}s\".format(time.time() - start))\n",
    "    print(\"F1 score\", val_acc)\n",
    "    return train_loss / len(train_loader)\n",
    "\n",
    "\n",
    "# 模型验证\n",
    "def validate(val_loader, model):\n",
    "    model.eval()\n",
    "    val_acc = 0.0\n",
    "    preds = torch.tensor([])\n",
    "    target_all = torch.tensor([])\n",
    "    with torch.no_grad():\n",
    "        for i, (input, target) in enumerate(val_loader):\n",
    "            input = input.to(device)\n",
    "            target = target.to(device)\n",
    "            output = model(input).cpu()\n",
    "            # val_acc += (output.argmax(1) == target).sum().item()\n",
    "            preds = torch.cat((preds, output.argmax(1)))\n",
    "            target_all = torch.cat((target_all, target.cpu()))\n",
    "\n",
    "        val_acc = torchmetrics.functional.classification.multiclass_f1_score(\n",
    "            preds, target_all, num_classes=25, average=\"macro\"\n",
    "        )\n",
    "\n",
    "    # return val_acc / len(val_loader.dataset)\n",
    "    return val_acc\n",
    "\n",
    "\n",
    "# 模型预测\n",
    "def predict(test_loader, model):\n",
    "    model.eval()\n",
    "\n",
    "    test_pred = []\n",
    "    with torch.no_grad():\n",
    "        for i, (input, target) in enumerate(test_loader):\n",
    "            input = input.to(device)\n",
    "            target = target.to(device)\n",
    "            output = model(input)\n",
    "            test_pred.append(output.data.cpu().numpy())\n",
    "\n",
    "    return np.vstack(test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 2.2976112365722656 t=16.140373468399048s\n",
      "Train loss 2.1149075031280518 t=16.00533151626587s\n",
      "Train loss 1.7303135395050049 t=16.142765045166016s\n",
      "Train loss 1.8303672075271606 t=16.645331144332886s\n",
      "Train loss 1.4413533210754395 t=16.39311146736145s\n",
      "Train loss 1.281042456626892 t=17.26898956298828s\n",
      "Train loss 1.3583972454071045 t=17.249473094940186s\n",
      "t=121.25433492660522s\n",
      "F1 score tensor(0.4267)\n",
      "epoch 1 : train_loss: 1.744115650490762 val_f1: tensor(0.5191)\n",
      "Train loss 0.8689115643501282 t=16.13426971435547s\n",
      "Train loss 0.9837633371353149 t=15.964512825012207s\n",
      "Train loss 0.9596181511878967 t=16.01486325263977s\n",
      "Train loss 1.0082253217697144 t=15.850048780441284s\n",
      "Train loss 0.839529275894165 t=15.899014949798584s\n",
      "Train loss 0.3169907331466675 t=15.859082460403442s\n",
      "Train loss 0.5702284574508667 t=15.73872709274292s\n",
      "t=116.4113380908966s\n",
      "F1 score tensor(0.7129)\n",
      "epoch 1 : train_loss: 0.8117695489921257 val_f1: tensor(0.7750)\n",
      "Train loss 0.3554437756538391 t=15.643813133239746s\n",
      "Train loss 0.34820932149887085 t=15.777693748474121s\n",
      "Train loss 0.39432817697525024 t=15.60913896560669s\n",
      "Train loss 0.9753468632698059 t=15.781788110733032s\n",
      "Train loss 0.4702950716018677 t=15.979589939117432s\n",
      "Train loss 0.30670467019081116 t=15.803683757781982s\n",
      "Train loss 0.9794008731842041 t=16.313228845596313s\n",
      "t=116.09097695350647s\n",
      "F1 score tensor(0.8067)\n",
      "epoch 1 : train_loss: 0.5257095246182095 val_f1: tensor(0.7866)\n",
      "Train loss 0.8271006941795349 t=15.643986940383911s\n",
      "Train loss 0.23932108283042908 t=15.403189182281494s\n",
      "Train loss 0.2979266941547394 t=15.264313697814941s\n",
      "Train loss 0.3979884684085846 t=15.234161376953125s\n",
      "Train loss 0.24189166724681854 t=15.394582271575928s\n",
      "Train loss 0.5194761157035828 t=15.57694697380066s\n",
      "Train loss 0.11103353649377823 t=15.805114507675171s\n",
      "t=113.22405457496643s\n",
      "F1 score tensor(0.8479)\n",
      "epoch 1 : train_loss: 0.39406293076138166 val_f1: tensor(0.8364)\n",
      "Train loss 0.2634960114955902 t=15.564298391342163s\n",
      "Train loss 0.5908827185630798 t=15.706496238708496s\n",
      "Train loss 0.1267838180065155 t=15.419156789779663s\n",
      "Train loss 0.273987740278244 t=15.601263523101807s\n",
      "Train loss 0.4135894477367401 t=15.595276117324829s\n",
      "Train loss 0.2536391615867615 t=15.402108907699585s\n",
      "Train loss 0.35941359400749207 t=15.381771087646484s\n",
      "t=113.47811675071716s\n",
      "F1 score tensor(0.8758)\n",
      "epoch 1 : train_loss: 0.31508804172807453 val_f1: tensor(0.8367)\n",
      "Train loss 0.5011641383171082 t=15.248821496963501s\n",
      "Train loss 0.3874286115169525 t=15.354706764221191s\n",
      "Train loss 0.3841586410999298 t=15.386756420135498s\n",
      "Train loss 0.34508198499679565 t=15.385818481445312s\n",
      "Train loss 0.13048915565013885 t=15.395636558532715s\n",
      "Train loss 0.2274066060781479 t=15.395896911621094s\n",
      "Train loss 0.3449976146221161 t=15.383760929107666s\n",
      "t=112.36941123008728s\n",
      "F1 score tensor(0.8877)\n",
      "epoch 1 : train_loss: 0.27152236623921405 val_f1: tensor(0.8650)\n",
      "Train loss 0.13597717881202698 t=15.247259140014648s\n",
      "Train loss 0.40507712960243225 t=15.39490270614624s\n",
      "Train loss 0.2619422376155853 t=15.409831762313843s\n",
      "Train loss 0.25166091322898865 t=15.404386281967163s\n",
      "Train loss 0.15000943839550018 t=15.498071670532227s\n",
      "Train loss 0.13879325985908508 t=15.435376405715942s\n",
      "Train loss 0.12051195651292801 t=15.440302610397339s\n",
      "t=112.65848898887634s\n",
      "F1 score tensor(0.8987)\n",
      "epoch 1 : train_loss: 0.2410995006031279 val_f1: tensor(0.8649)\n"
     ]
    }
   ],
   "source": [
    "epochs = 0\n",
    "last_acc = 0\n",
    "val_acc = 0.001\n",
    "while last_acc < val_acc:\n",
    "    last_acc = val_acc\n",
    "    train_loss = train(train_loader, model, criterion, optimizer)\n",
    "    val_acc = validate(val_loader, model)\n",
    "    # train_acc = validate(train_loader, model)\n",
    "    print(\n",
    "        \"epoch {} :\".format(epochs + 1), \"train_loss:\", train_loss, \"val_f1:\", val_acc\n",
    "    )\n",
    "    epochs += 1\n",
    "    if val_acc>last_acc:\n",
    "        torch.save(model.state_dict(),\"./model/baseline.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# 对测试集多次预测\n",
    "pred = None\n",
    "model.load_state_dict(torch.load(\"./model/baseline.pth\"))\n",
    "for _ in range(3):\n",
    "    if pred is None:\n",
    "        pred = predict(test_loader, model)\n",
    "    else:\n",
    "        pred += predict(test_loader, model)\n",
    "    print(_+1)\n",
    "submit = pd.DataFrame(\n",
    "    {\n",
    "        'name': [x.split('/')[-1] for x in test_path],\n",
    "        'label': pred.argmax(1)\n",
    "})\n",
    "\n",
    "# 生成提交结果\n",
    "submit = submit.sort_values(by='name')\n",
    "submit.to_csv('submit.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "train_loss = 0.0\n",
    "model = model.to(device)\n",
    "for i, (input, target) in enumerate(train_loader):\n",
    "    input = input.to(device)\n",
    "    target = target.to(device)\n",
    "    output = model(input)\n",
    "    loss = criterion(output, target)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "model.fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_acc = validate(val_loader, model)\n",
    "val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
