{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# !pip install torchmetrics\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "okg3ZFDwn02Z",
        "outputId": "c71b6c61-1fc1-4ad4-ac32-6000aebcba5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.10/dist-packages (1.0.3)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.23.5)\n",
            "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.0.1+cu118)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (23.1)\n",
            "Requirement already satisfied: lightning-utilities>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (0.9.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.7.0->torchmetrics) (4.7.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.1->torchmetrics) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.1->torchmetrics) (16.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.1->torchmetrics) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.1->torchmetrics) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XqHMqXFjnl4d"
      },
      "outputs": [],
      "source": [
        "import os, sys, glob, argparse\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "# from tqdm import tqdm\n",
        "\n",
        "import cv2, time\n",
        "# from PIL import Image\n",
        "# from sklearn.model_selection import train_test_split, StratifiedKFold, KFold\n",
        "\n",
        "import torch\n",
        "torch.manual_seed(0)\n",
        "torch.backends.cudnn.deterministic = False\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "import torchmetrics\n",
        "import torchvision.models as models\n",
        "# import torchvision.transforms as transforms\n",
        "# import torchvision.datasets as datasets\n",
        "import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "# import torch.optim as optim\n",
        "# from torch.autograd import Variable\n",
        "from torch.utils.data.dataset import Dataset\n",
        "\n",
        "\n",
        "# Check if GPU is available\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "2_g72k1hnl4g",
        "outputId": "760f54d3-712c-4015-da19-829abab0d5dd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Tesla T4'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "torch.cuda.get_device_name(0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Hpeox/AI_Camp.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3rUR6TbRoQlK",
        "outputId": "96ae4231-cd1f-4248-d441-406e05900fc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'AI_Camp'...\n",
            "remote: Enumerating objects: 29073, done.\u001b[K\n",
            "remote: Counting objects: 100% (1899/1899), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1793/1793), done.\u001b[K\n",
            "remote: Total 29073 (delta 100), reused 1895 (delta 97), pack-reused 27174\u001b[K\n",
            "Receiving objects: 100% (29073/29073), 237.79 MiB | 16.70 MiB/s, done.\n",
            "Resolving deltas: 100% (101/101), done.\n",
            "Updating files: 100% (30609/30609), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Km-laM3cpJCC",
        "outputId": "d6d9775c-2cb6-418a-b378-c31ce4399ef2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AI_Camp  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tp1ajNpznl4h"
      },
      "outputs": [],
      "source": [
        "# 读取数据集\n",
        "train_path = glob.glob(\"/content/AI_Camp/CNN/data/train/*\")\n",
        "test_path = glob.glob(\"/content/AI_Camp/CNN/data/test/*\")\n",
        "\n",
        "train_path.sort()\n",
        "test_path.sort()\n",
        "\n",
        "train_df = pd.read_csv(\"/content/AI_Camp/CNN/data/train.csv\")\n",
        "train_df = train_df.sort_values(by=\"name\")\n",
        "train_label = train_df[\"label\"].values\n",
        "\n",
        "# 自定义数据集\n",
        "# 带有图片缓存的逻辑\n",
        "DATA_CACHE = {}\n",
        "\n",
        "\n",
        "class XunFeiDataset(Dataset):\n",
        "    def __init__(self, img_path, img_label, transform=None):\n",
        "        self.img_path = img_path\n",
        "        self.img_label = img_label\n",
        "        if transform is not None:\n",
        "            self.transform = transform\n",
        "        else:\n",
        "            self.transform = None\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        if self.img_path[index] in DATA_CACHE:\n",
        "            img = DATA_CACHE[self.img_path[index]]\n",
        "        else:\n",
        "            img = cv2.imread(self.img_path[index])\n",
        "            DATA_CACHE[self.img_path[index]] = img\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(image=img)[\"image\"]\n",
        "        img = img.transpose([2, 0, 1])\n",
        "        return img, torch.from_numpy(np.array(self.img_label[index]))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_path)\n",
        "\n",
        "\n",
        "import albumentations as A\n",
        "\n",
        "dataset = XunFeiDataset(\n",
        "    train_path,\n",
        "    train_label,\n",
        "    A.Compose(\n",
        "        [\n",
        "            A.RandomRotate90(),\n",
        "            A.Resize(256, 256),\n",
        "            A.RandomCrop(224, 224),\n",
        "            A.HorizontalFlip(p=0.5),\n",
        "            A.RandomBrightnessContrast(p=0.5),\n",
        "            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "        ]\n",
        "    ),\n",
        ")\n",
        "train_dataset, test_dataset = torch.utils.data.random_split(\n",
        "    dataset=dataset, lengths=[0.95, 0.05], generator=torch.Generator().manual_seed(42)\n",
        ")\n",
        "\n",
        "# 训练集\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset, batch_size=32, shuffle=True, num_workers=0, pin_memory=False\n",
        ")\n",
        "\n",
        "# 验证集\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    test_dataset, batch_size=32, shuffle=False, num_workers=0, pin_memory=False\n",
        ")\n",
        "\n",
        "# 测试集\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    XunFeiDataset(\n",
        "        test_path,\n",
        "        [0] * len(test_path),\n",
        "        A.Compose(\n",
        "            [\n",
        "                A.Resize(256, 256),\n",
        "                A.RandomCrop(224, 224),\n",
        "                A.HorizontalFlip(p=0.5),\n",
        "                # A.RandomBrightnessContrast(p=0.5),\n",
        "                A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "            ]\n",
        "        ),\n",
        "    ),\n",
        "    batch_size=50,\n",
        "    shuffle=False,\n",
        "    num_workers=0,\n",
        "    pin_memory=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ov6UeG2qnl4k",
        "outputId": "aa880508-81bf-4272-d28e-2e1987d66e25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n",
            "100%|██████████| 97.8M/97.8M [00:01<00:00, 83.8MB/s]\n",
            "Downloading: \"https://download.pytorch.org/models/resnet101-cd907fc2.pth\" to /root/.cache/torch/hub/checkpoints/resnet101-cd907fc2.pth\n",
            "100%|██████████| 171M/171M [00:00<00:00, 221MB/s]\n",
            "Downloading: \"https://download.pytorch.org/models/resnet152-f82ba261.pth\" to /root/.cache/torch/hub/checkpoints/resnet152-f82ba261.pth\n",
            "100%|██████████| 230M/230M [00:04<00:00, 56.0MB/s]\n"
          ]
        }
      ],
      "source": [
        "class XunFeiNet50(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(XunFeiNet50, self).__init__()\n",
        "        model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
        "        model.avgpool = nn.AdaptiveAvgPool2d(1)\n",
        "        model.fc = nn.Linear(2048, 25)\n",
        "        self.resnet = model\n",
        "    def forward(self, img):\n",
        "        out = self.resnet(img)\n",
        "        return out\n",
        "\n",
        "class XunFeiNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(XunFeiNet, self).__init__()\n",
        "        model = models.resnet101(weights=models.ResNet101_Weights.DEFAULT)\n",
        "        model.avgpool = nn.AdaptiveAvgPool2d(1)\n",
        "        model.fc = nn.Linear(2048, 25)\n",
        "        self.resnet = model\n",
        "    def forward(self, img):\n",
        "        out = self.resnet(img)\n",
        "        return out\n",
        "class XunFeiNet152(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(XunFeiNet152, self).__init__()\n",
        "        model = models.resnet152(weights=models.ResNet152_Weights.DEFAULT)\n",
        "        model.avgpool = nn.AdaptiveAvgPool2d(1)\n",
        "        model.fc = nn.Linear(2048, 25)\n",
        "        self.resnet = model\n",
        "    def forward(self, img):\n",
        "        out = self.resnet(img)\n",
        "        return out\n",
        "model50 = XunFeiNet50().to(device)\n",
        "model101= XunFeiNet().to(device)\n",
        "model152= XunFeiNet152().to(device)\n",
        "# model = model.to(device)\n",
        "criterion = nn.CrossEntropyLoss().cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p_ES2vVtnl4l"
      },
      "outputs": [],
      "source": [
        "# 模型训练\n",
        "def train(train_loader, model, criterion, optimizer):\n",
        "    start = time.time()\n",
        "    start_batch = [start, 0]\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    preds = torch.tensor([])\n",
        "    target_all = torch.tensor([])\n",
        "    for i, (input, target) in enumerate(train_loader):\n",
        "        input = input.to(device)\n",
        "        target = target.to(device)\n",
        "        output = model(input)\n",
        "        loss = criterion(output, target)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i + 1) % 100 == 0:\n",
        "            start_batch[((i + 1) // 100) % 2] = time.time()\n",
        "            print(\n",
        "                \"Train loss\",\n",
        "                loss.item(),\n",
        "                \"t={}s\".format(\n",
        "                    start_batch[((i + 1) // 100) % 2]\n",
        "                    - start_batch[(1 + (i + 1) // 100) % 2]\n",
        "                ),\n",
        "            )\n",
        "\n",
        "        preds = torch.cat((preds, output.cpu().argmax(1)))\n",
        "        target_all = torch.cat((target_all, target.cpu()))\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    val_acc = torchmetrics.functional.classification.multiclass_f1_score(\n",
        "        preds, target_all, num_classes=25, average=\"macro\"\n",
        "    )\n",
        "    print(\"t={}s\".format(time.time() - start))\n",
        "    print(\"F1 score\", val_acc)\n",
        "    return train_loss / len(train_loader)\n",
        "\n",
        "\n",
        "# 模型验证\n",
        "def validate(val_loader, model, criterion):\n",
        "    model.eval()\n",
        "    val_acc = 0.0\n",
        "    preds = torch.tensor([])\n",
        "    target_all = torch.tensor([])\n",
        "    val_loss=0.0\n",
        "    with torch.no_grad():\n",
        "        for i, (input, target) in enumerate(val_loader):\n",
        "            input = input.to(device)\n",
        "            target = target.to(device)\n",
        "            output = model(input)\n",
        "            loss = criterion(output, target)\n",
        "            # val_acc += (output.argmax(1) == target).sum().item()\n",
        "            preds = torch.cat((preds, output.cpu().argmax(1)))\n",
        "            target_all = torch.cat((target_all, target.cpu()))\n",
        "\n",
        "        val_acc = torchmetrics.functional.classification.multiclass_f1_score(\n",
        "            preds, target_all, num_classes=25, average=\"macro\"\n",
        "        )\n",
        "        val_loss += loss.item()\n",
        "    # return val_acc / len(val_loader.dataset)\n",
        "    return val_acc,val_loss/len(val_loader)\n",
        "\n",
        "\n",
        "# 模型预测\n",
        "def predict(test_loader, model):\n",
        "    model.eval()\n",
        "\n",
        "    test_pred = []\n",
        "    with torch.no_grad():\n",
        "        for i, (input, target) in enumerate(test_loader):\n",
        "            input = input.to(device)\n",
        "            output = model(input)\n",
        "            test_pred.append(output.data.cpu().numpy())\n",
        "\n",
        "    return np.vstack(test_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fHTYjp9-nl4m",
        "outputId": "e14245ce-54f4-47dd-eb4e-608e926211c7"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train loss 3.0476365089416504 t=88.26209783554077s\n",
            "Train loss 2.0708141326904297 t=77.12735915184021s\n",
            "Train loss 1.7472223043441772 t=77.31742024421692s\n",
            "Train loss 1.7517409324645996 t=76.69264245033264s\n",
            "Train loss 1.0861563682556152 t=76.21940469741821s\n",
            "Train loss 1.1610586643218994 t=75.78902220726013s\n",
            "t=534.5744822025299s\n",
            "F1 score tensor(0.3917)\n",
            "epoch 1 : train_loss: 1.9450501847722281 val_loss: 0.03508191969659594 val_f1: tensor(0.6550)\n",
            "Train loss 1.1151342391967773 t=74.02593159675598s\n",
            "Train loss 0.7412101626396179 t=74.27640891075134s\n",
            "Train loss 0.8763712644577026 t=74.3703544139862s\n",
            "Train loss 0.9524921774864197 t=74.46224045753479s\n",
            "Train loss 0.6785811185836792 t=74.34841799736023s\n",
            "Train loss 0.9604212045669556 t=74.33931946754456s\n",
            "t=505.7985739707947s\n",
            "F1 score tensor(0.7083)\n",
            "epoch 2 : train_loss: 0.8310266115241814 val_loss: 0.02287891341580285 val_f1: tensor(0.7724)\n",
            "Train loss 0.3797929286956787 t=74.01039528846741s\n",
            "Train loss 0.5162652134895325 t=74.4267566204071s\n",
            "Train loss 0.5350441932678223 t=74.32329058647156s\n",
            "Train loss 0.574765145778656 t=74.32813382148743s\n",
            "Train loss 0.33936429023742676 t=74.56576466560364s\n",
            "Train loss 0.42396944761276245 t=74.37207412719727s\n",
            "t=506.09827995300293s\n",
            "F1 score tensor(0.8012)\n",
            "epoch 3 : train_loss: 0.5478706821339772 val_loss: 0.021649120582474604 val_f1: tensor(0.8241)\n",
            "Train loss 0.5820589065551758 t=73.96980667114258s\n",
            "Train loss 0.5966716408729553 t=74.5930528640747s\n",
            "Train loss 0.16715183854103088 t=74.45656323432922s\n",
            "Train loss 0.4228383004665375 t=74.48730659484863s\n",
            "Train loss 0.4344993531703949 t=74.45028114318848s\n",
            "Train loss 0.46450790762901306 t=74.23673439025879s\n",
            "t=506.32208466529846s\n",
            "F1 score tensor(0.8363)\n",
            "epoch 4 : train_loss: 0.42639634617099675 val_loss: 0.009494867589738634 val_f1: tensor(0.8500)\n",
            "Train loss 0.4295846223831177 t=74.00876903533936s\n",
            "Train loss 0.5325315594673157 t=74.51792931556702s\n",
            "Train loss 0.33595597743988037 t=74.57101464271545s\n",
            "Train loss 0.31224673986434937 t=74.31554579734802s\n",
            "Train loss 0.3296903073787689 t=74.44833016395569s\n",
            "Train loss 0.2578333616256714 t=74.54148292541504s\n",
            "t=506.44781970977783s\n",
            "F1 score tensor(0.8620)\n",
            "epoch 5 : train_loss: 0.3488009959875049 val_loss: 0.012350549300511679 val_f1: tensor(0.8548)\n",
            "Train loss 0.3686387836933136 t=74.04862666130066s\n",
            "Train loss 0.17164531350135803 t=74.44398951530457s\n",
            "Train loss 0.33749982714653015 t=74.42057847976685s\n",
            "Train loss 0.5357812643051147 t=74.35212826728821s\n",
            "Train loss 0.2221008986234665 t=74.3741466999054s\n",
            "Train loss 0.34721121191978455 t=74.57899522781372s\n",
            "t=506.2647078037262s\n",
            "F1 score tensor(0.8778)\n",
            "epoch 6 : train_loss: 0.306266046119594 val_loss: 0.010805476870801713 val_f1: tensor(0.8691)\n",
            "Train loss 0.22792604565620422 t=73.75588941574097s\n",
            "Train loss 0.3033406436443329 t=74.10757207870483s\n",
            "Train loss 0.14312614500522614 t=73.98926711082458s\n",
            "Train loss 0.2561994194984436 t=73.8850269317627s\n",
            "Train loss 0.2861427366733551 t=74.06996035575867s\n",
            "Train loss 0.231609046459198 t=73.97423267364502s\n",
            "t=503.5283441543579s\n",
            "F1 score tensor(0.8876)\n",
            "epoch 7 : train_loss: 0.27424107553784716 val_loss: 0.011073919634024302 val_f1: tensor(0.8604)\n"
          ]
        }
      ],
      "source": [
        "# for best model\n",
        "model=model152\n",
        "optimizer = torch.optim.AdamW(model.parameters(), 0.0001)\n",
        "# model=model101\n",
        "epochs = 0\n",
        "last_acc = 0\n",
        "val_acc = 0.001\n",
        "while last_acc < val_acc:\n",
        "    last_acc = val_acc\n",
        "    train_loss = train(train_loader, model, criterion, optimizer)\n",
        "    val_acc,val_loss = validate(val_loader, model, criterion)\n",
        "    # train_acc = validate(train_loader, model)\n",
        "    print(\n",
        "        \"epoch {} :\".format(epochs + 1), \"train_loss:\", train_loss, 'val_loss:', val_loss,\"val_f1:\", val_acc\n",
        "    )\n",
        "    epochs += 1\n",
        "    if val_acc>last_acc:\n",
        "        torch.save(model.state_dict(),\"/content/model/baseline_152.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "katEJQbInl4n",
        "outputId": "1fa99f7b-22a1-4f9f-d4b7-72482b732561"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 1.9190107583999634 t=55.2235164642334s\n",
            "Train loss 1.604310154914856 t=54.169617891311646s\n",
            "Train loss 1.5684213638305664 t=54.4313588142395s\n",
            "Train loss 1.1803944110870361 t=54.01967525482178s\n",
            "Train loss 1.21957528591156 t=54.55041790008545s\n",
            "Train loss 0.7174181342124939 t=54.105995416641235s\n",
            "t=370.2995150089264s\n",
            "F1 score tensor(0.4827)\n",
            "epoch 1 : train_loss: 1.5851940297345233 val_loss: 0.020687128106753033 val_f1: tensor(0.6197)\n",
            "Train loss 0.7935527563095093 t=53.69859838485718s\n",
            "Train loss 0.5381841659545898 t=54.11372089385986s\n",
            "Train loss 0.8454071283340454 t=54.05889296531677s\n",
            "Train loss 0.7152472138404846 t=53.9410765171051s\n",
            "Train loss 0.40476733446121216 t=54.28064298629761s\n",
            "Train loss 0.5825835466384888 t=54.00325417518616s\n",
            "t=367.86373949050903s\n",
            "F1 score tensor(0.7225)\n",
            "epoch 2 : train_loss: 0.801320823716347 val_loss: 0.019692533546023898 val_f1: tensor(0.7765)\n",
            "Train loss 0.4299902617931366 t=53.609546184539795s\n",
            "Train loss 0.6208288073539734 t=54.09341382980347s\n",
            "Train loss 0.4286290407180786 t=54.10776615142822s\n",
            "Train loss 0.5641669034957886 t=54.021806478500366s\n",
            "Train loss 0.5953429341316223 t=54.20747995376587s\n",
            "Train loss 0.6751061677932739 t=53.915839433670044s\n",
            "t=367.7244074344635s\n",
            "F1 score tensor(0.7966)\n",
            "epoch 3 : train_loss: 0.5541409031978963 val_loss: 0.017324558562702604 val_f1: tensor(0.8071)\n",
            "Train loss 0.4470987617969513 t=53.729098081588745s\n",
            "Train loss 0.45614093542099 t=54.137688398361206s\n",
            "Train loss 0.6947254538536072 t=54.07289528846741s\n",
            "Train loss 0.2178269922733307 t=53.836676597595215s\n",
            "Train loss 0.571686863899231 t=54.03188490867615s\n",
            "Train loss 0.5606047511100769 t=53.979154109954834s\n",
            "t=367.6601836681366s\n",
            "F1 score tensor(0.8332)\n",
            "epoch 4 : train_loss: 0.4441464406803316 val_loss: 0.006949861844380696 val_f1: tensor(0.8464)\n",
            "Train loss 0.39484477043151855 t=53.551114559173584s\n",
            "Train loss 0.3278876543045044 t=53.961281299591064s\n",
            "Train loss 0.24238945543766022 t=53.92502999305725s\n",
            "Train loss 0.4875499904155731 t=54.11970639228821s\n",
            "Train loss 0.1718786358833313 t=54.02476358413696s\n",
            "Train loss 0.42219823598861694 t=53.90390729904175s\n",
            "t=367.1305661201477s\n",
            "F1 score tensor(0.8567)\n",
            "epoch 5 : train_loss: 0.36780557778175704 val_loss: 0.012049986256493462 val_f1: tensor(0.8350)\n"
          ]
        }
      ],
      "source": [
        "# for best model\n",
        "model=model101\n",
        "optimizer = torch.optim.AdamW(model.parameters(), 0.0001)\n",
        "# model=model101\n",
        "epochs = 0\n",
        "last_acc = 0\n",
        "val_acc = 0.001\n",
        "while last_acc < val_acc:\n",
        "    last_acc = val_acc\n",
        "    train_loss = train(train_loader, model, criterion, optimizer)\n",
        "    val_acc,val_loss = validate(val_loader, model, criterion)\n",
        "    # train_acc = validate(train_loader, model)\n",
        "    print(\n",
        "        \"epoch {} :\".format(epochs + 1), \"train_loss:\", train_loss, 'val_loss:', val_loss,\"val_f1:\", val_acc\n",
        "    )\n",
        "    epochs += 1\n",
        "    if val_acc>last_acc:\n",
        "        torch.save(model.state_dict(),\"./model/baseline_101.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJUCZD4Qnl4p",
        "outputId": "0c7e5985-49ad-4f77-b3f3-f63083a15c5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 3.0778005123138428 t=36.45119881629944s\n",
            "Train loss 2.9839706420898438 t=34.90014696121216s\n",
            "Train loss 2.4677653312683105 t=34.40766882896423s\n",
            "Train loss 1.9801123142242432 t=35.00399112701416s\n",
            "Train loss 1.7957130670547485 t=35.91481256484985s\n",
            "Train loss 1.6115543842315674 t=35.30828809738159s\n",
            "t=240.06659722328186s\n",
            "F1 score tensor(0.2579)\n",
            "epoch 1 : train_loss: 2.3984916362468365 val_loss: 0.03880749146143595 val_f1: tensor(0.4967)\n",
            "Train loss 1.3293904066085815 t=34.67344927787781s\n",
            "Train loss 1.2297985553741455 t=34.67178559303284s\n",
            "Train loss 1.791744351387024 t=34.863085985183716s\n",
            "Train loss 0.9549806118011475 t=34.56373620033264s\n",
            "Train loss 1.0414737462997437 t=34.538920402526855s\n",
            "Train loss 0.9631244540214539 t=34.403910636901855s\n",
            "t=235.643319606781s\n",
            "F1 score tensor(0.6050)\n",
            "epoch 2 : train_loss: 1.1759235292629349 val_loss: 0.020151206188731723 val_f1: tensor(0.6879)\n",
            "Train loss 0.9067736864089966 t=34.287184953689575s\n",
            "Train loss 1.1491647958755493 t=34.702731132507324s\n",
            "Train loss 0.9758962392807007 t=34.54637122154236s\n",
            "Train loss 0.546239972114563 t=34.40998554229736s\n",
            "Train loss 0.9139672517776489 t=34.52645921707153s\n",
            "Train loss 0.844455897808075 t=34.47757411003113s\n",
            "t=234.79713034629822s\n",
            "F1 score tensor(0.7354)\n",
            "epoch 3 : train_loss: 0.7678721320436565 val_loss: 0.013953881131278144 val_f1: tensor(0.7578)\n",
            "Train loss 0.5000051856040955 t=34.163233518600464s\n",
            "Train loss 0.7704934477806091 t=34.64043426513672s\n",
            "Train loss 0.3846789300441742 t=34.40974473953247s\n",
            "Train loss 0.6695016026496887 t=34.69147038459778s\n",
            "Train loss 0.44525301456451416 t=34.48703217506409s\n",
            "Train loss 0.34976571798324585 t=34.448720932006836s\n",
            "t=234.70285272598267s\n",
            "F1 score tensor(0.8012)\n",
            "epoch 4 : train_loss: 0.5613221662466397 val_loss: 0.011797895034154257 val_f1: tensor(0.8044)\n",
            "Train loss 0.48889216780662537 t=34.19235682487488s\n",
            "Train loss 0.4370609223842621 t=34.64817547798157s\n",
            "Train loss 0.366258829832077 t=34.47136926651001s\n",
            "Train loss 1.0782132148742676 t=34.507429122924805s\n",
            "Train loss 0.5624903440475464 t=34.417182207107544s\n",
            "Train loss 0.45055827498435974 t=34.43978214263916s\n",
            "t=234.53874492645264s\n",
            "F1 score tensor(0.8316)\n",
            "epoch 5 : train_loss: 0.45293502362989296 val_loss: 0.011781370474232567 val_f1: tensor(0.8111)\n",
            "Train loss 0.26153314113616943 t=34.46519660949707s\n",
            "Train loss 0.3385913670063019 t=34.83457088470459s\n",
            "Train loss 0.3995102047920227 t=34.466076374053955s\n",
            "Train loss 0.5067423582077026 t=34.523284912109375s\n",
            "Train loss 0.3800930380821228 t=34.5570342540741s\n",
            "Train loss 0.2346435934305191 t=34.59206533432007s\n",
            "t=235.38150572776794s\n",
            "F1 score tensor(0.8515)\n",
            "epoch 6 : train_loss: 0.38528356503109923 val_loss: 0.011768250001801385 val_f1: tensor(0.8508)\n",
            "Train loss 0.2236078679561615 t=34.062705993652344s\n",
            "Train loss 0.3849661648273468 t=34.68785500526428s\n",
            "Train loss 0.3555167317390442 t=34.526718854904175s\n",
            "Train loss 0.3750046193599701 t=34.46859312057495s\n",
            "Train loss 0.3910205662250519 t=34.605201959609985s\n",
            "Train loss 0.1959223449230194 t=34.50557327270508s\n",
            "t=235.48985409736633s\n",
            "F1 score tensor(0.8712)\n",
            "epoch 7 : train_loss: 0.3370479986990601 val_loss: 0.0056138791971736485 val_f1: tensor(0.8539)\n",
            "Train loss 0.19347406923770905 t=34.244460344314575s\n",
            "Train loss 0.1683834344148636 t=34.61076998710632s\n",
            "Train loss 0.29168564081192017 t=34.662272930145264s\n",
            "Train loss 0.18336403369903564 t=34.727986335754395s\n",
            "Train loss 0.7308184504508972 t=34.785677671432495s\n",
            "Train loss 0.3287404775619507 t=34.754170179367065s\n",
            "t=235.82835340499878s\n",
            "F1 score tensor(0.8796)\n",
            "epoch 8 : train_loss: 0.2996805245918324 val_loss: 0.01422888371679518 val_f1: tensor(0.8684)\n",
            "Train loss 0.2143174707889557 t=34.37391901016235s\n",
            "Train loss 0.18909713625907898 t=34.894301414489746s\n",
            "Train loss 0.22819651663303375 t=34.53401041030884s\n",
            "Train loss 0.3202834725379944 t=34.70507836341858s\n",
            "Train loss 0.4586082100868225 t=34.655789613723755s\n",
            "Train loss 0.1507870852947235 t=34.83985257148743s\n",
            "t=235.99119687080383s\n",
            "F1 score tensor(0.8861)\n",
            "epoch 9 : train_loss: 0.2820986081543823 val_loss: 0.0106805306341913 val_f1: tensor(0.8705)\n",
            "Train loss 0.15127559006214142 t=34.613502740859985s\n",
            "Train loss 0.20421569049358368 t=34.59429311752319s\n",
            "Train loss 0.22219054400920868 t=34.49687218666077s\n",
            "Train loss 0.20719949901103973 t=34.67625284194946s\n",
            "Train loss 0.14817428588867188 t=34.704829931259155s\n",
            "Train loss 0.1391613781452179 t=34.895933866500854s\n",
            "t=236.00272631645203s\n",
            "F1 score tensor(0.8915)\n",
            "epoch 10 : train_loss: 0.26102879466748796 val_loss: 0.010372487207253775 val_f1: tensor(0.8545)\n"
          ]
        }
      ],
      "source": [
        "# for best model\n",
        "model=model50\n",
        "optimizer = torch.optim.AdamW(model.parameters(), 0.0001)\n",
        "# model=model101\n",
        "epochs = 0\n",
        "last_acc = 0\n",
        "val_acc = 0.001\n",
        "while last_acc < val_acc:\n",
        "    last_acc = val_acc\n",
        "    train_loss = train(train_loader, model, criterion, optimizer)\n",
        "    val_acc,val_loss = validate(val_loader, model, criterion)\n",
        "    # train_acc = validate(train_loader, model)\n",
        "    print(\n",
        "        \"epoch {} :\".format(epochs + 1), \"train_loss:\", train_loss, 'val_loss:', val_loss,\"val_f1:\", val_acc\n",
        "    )\n",
        "    epochs += 1\n",
        "    if val_acc>last_acc:\n",
        "        torch.save(model.state_dict(),\"./model/baseline_50.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BWWDqqkFnl4q",
        "outputId": "83c53b42-764e-4fdc-e656-36ebc8cf4a33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth\n",
            "100%|██████████| 83.3M/83.3M [00:01<00:00, 61.5MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 2.675316095352173 t=21.672868013381958s\n",
            "Train loss 1.9864163398742676 t=20.929474592208862s\n",
            "Train loss 2.0389387607574463 t=19.96168303489685s\n",
            "Train loss 1.6547023057937622 t=20.795820713043213s\n",
            "Train loss 1.5560764074325562 t=20.140869140625s\n",
            "Train loss 1.2593016624450684 t=20.451364755630493s\n",
            "t=140.70986008644104s\n",
            "F1 score tensor(0.4035)\n",
            "epoch 1 : train_loss: 1.8789681092933062 val_loss: 0.02416436539755927 val_f1: tensor(0.5753)\n",
            "Train loss 1.5519540309906006 t=20.127243280410767s\n",
            "Train loss 1.2608009576797485 t=20.482006788253784s\n",
            "Train loss 0.9310277104377747 t=20.065386056900024s\n",
            "Train loss 0.8443719148635864 t=20.39361023902893s\n",
            "Train loss 1.356546401977539 t=20.442185163497925s\n",
            "Train loss 0.8738254308700562 t=20.57886028289795s\n",
            "t=138.82112073898315s\n",
            "F1 score tensor(0.6479)\n",
            "epoch 2 : train_loss: 1.045486842877014 val_loss: 0.024244331651263766 val_f1: tensor(0.6599)\n",
            "Train loss 0.6981716156005859 t=20.582112550735474s\n",
            "Train loss 0.7801882028579712 t=20.508065938949585s\n",
            "Train loss 0.6403602361679077 t=20.485901355743408s\n",
            "Train loss 0.8444490432739258 t=20.007079362869263s\n",
            "Train loss 0.8450707793235779 t=20.778866052627563s\n",
            "Train loss 0.623715341091156 t=20.534644842147827s\n",
            "t=139.42924666404724s\n",
            "F1 score tensor(0.7304)\n",
            "epoch 3 : train_loss: 0.7785927804731237 val_loss: 0.023713522487216525 val_f1: tensor(0.7494)\n",
            "Train loss 0.39751800894737244 t=20.404422998428345s\n",
            "Train loss 0.6396536827087402 t=20.601869583129883s\n",
            "Train loss 1.059749722480774 t=20.37613010406494s\n",
            "Train loss 0.7347434759140015 t=20.573278188705444s\n",
            "Train loss 0.362239807844162 t=20.371524572372437s\n",
            "Train loss 0.9103720188140869 t=20.839212656021118s\n",
            "t=139.54546809196472s\n",
            "F1 score tensor(0.7706)\n",
            "epoch 4 : train_loss: 0.6443778600286633 val_loss: 0.0223098314470715 val_f1: tensor(0.7817)\n",
            "Train loss 0.8323338031768799 t=20.285807609558105s\n",
            "Train loss 0.43916842341423035 t=20.493436813354492s\n",
            "Train loss 0.5527456402778625 t=20.10206627845764s\n",
            "Train loss 0.6534532308578491 t=20.42827343940735s\n",
            "Train loss 0.5854504108428955 t=20.36669135093689s\n",
            "Train loss 0.6420873999595642 t=20.11024022102356s\n",
            "t=138.28935265541077s\n",
            "F1 score tensor(0.8023)\n",
            "epoch 5 : train_loss: 0.5445009368675571 val_loss: 0.01092070009973314 val_f1: tensor(0.7980)\n",
            "Train loss 0.6201298832893372 t=19.968762636184692s\n",
            "Train loss 0.29040104150772095 t=20.422539472579956s\n",
            "Train loss 0.6429813504219055 t=20.78671646118164s\n",
            "Train loss 0.6654389500617981 t=20.372032165527344s\n",
            "Train loss 0.3477614223957062 t=20.42935299873352s\n",
            "Train loss 0.5377129912376404 t=20.256693363189697s\n",
            "t=138.73031640052795s\n",
            "F1 score tensor(0.8172)\n",
            "epoch 6 : train_loss: 0.4971141429637147 val_loss: 0.016223720378345914 val_f1: tensor(0.8159)\n",
            "Train loss 0.48337310552597046 t=20.326428413391113s\n",
            "Train loss 0.6558368802070618 t=20.077703952789307s\n",
            "Train loss 0.8601675629615784 t=20.416189193725586s\n",
            "Train loss 0.6086755990982056 t=20.27799344062805s\n",
            "Train loss 0.46402785181999207 t=20.271454334259033s\n",
            "Train loss 0.624205470085144 t=20.355591535568237s\n",
            "t=138.01827001571655s\n",
            "F1 score tensor(0.8347)\n",
            "epoch 7 : train_loss: 0.44247650327749016 val_loss: 0.010729032258192698 val_f1: tensor(0.8081)\n"
          ]
        }
      ],
      "source": [
        "class XunFeiNet34(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(XunFeiNet34, self).__init__()\n",
        "        model = models.resnet34(weights=models.ResNet34_Weights.DEFAULT)\n",
        "        model.avgpool = nn.AdaptiveAvgPool2d(1)\n",
        "        model.fc = nn.Linear(512, 25)\n",
        "        self.resnet = model\n",
        "    def forward(self, img):\n",
        "        out = self.resnet(img)\n",
        "        return out\n",
        "model34 = XunFeiNet34().to(device)\n",
        "\n",
        "# for best model\n",
        "model=model34\n",
        "optimizer = torch.optim.AdamW(model.parameters(), 0.0001)\n",
        "# model=model101\n",
        "epochs = 0\n",
        "last_acc = 0\n",
        "val_acc = 0.001\n",
        "while last_acc < val_acc:\n",
        "    last_acc = val_acc\n",
        "    train_loss = train(train_loader, model, criterion, optimizer)\n",
        "    val_acc,val_loss = validate(val_loader, model, criterion)\n",
        "    # train_acc = validate(train_loader, model)\n",
        "    print(\n",
        "        \"epoch {} :\".format(epochs + 1), \"train_loss:\", train_loss, 'val_loss:', val_loss,\"val_f1:\", val_acc\n",
        "    )\n",
        "    epochs += 1\n",
        "    if val_acc>last_acc:\n",
        "        torch.save(model.state_dict(),\"./model/baseline_34.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ACt5g1c0nl4t",
        "outputId": "7256d3af-3dc1-46b6-f6f4-e125716a5e45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "2\n",
            "3\n",
            "1\n",
            "2\n",
            "3\n",
            "1\n",
            "2\n",
            "3\n",
            "1\n",
            "2\n",
            "3\n"
          ]
        }
      ],
      "source": [
        "# 对测试集多次预测\n",
        "pred = None\n",
        "model=model101\n",
        "model.load_state_dict(torch.load(\"./model/baseline_101.pth\"))\n",
        "for _ in range(3):\n",
        "    if pred is None:\n",
        "        pred = predict(test_loader, model)\n",
        "    else:\n",
        "        pred += predict(test_loader, model)\n",
        "    print(_+1)\n",
        "model=model152\n",
        "model.load_state_dict(torch.load(\"./model/baseline_152.pth\"))\n",
        "for _ in range(3):\n",
        "    if pred is None:\n",
        "        pred = predict(test_loader, model)\n",
        "    else:\n",
        "        pred += predict(test_loader, model)\n",
        "    print(_+1)\n",
        "model=model50\n",
        "model.load_state_dict(torch.load(\"./model/baseline_50.pth\"))\n",
        "for _ in range(3):\n",
        "    if pred is None:\n",
        "        pred = predict(test_loader, model)\n",
        "    else:\n",
        "        pred += 0.8*predict(test_loader, model)\n",
        "    print(_+1)\n",
        "model=model34\n",
        "model.load_state_dict(torch.load(\"./model/baseline_34.pth\"))\n",
        "for _ in range(3):\n",
        "    if pred is None:\n",
        "        pred = predict(test_loader, model)\n",
        "    else:\n",
        "        pred += 0.8*predict(test_loader, model)\n",
        "    print(_+1)\n",
        "submit = pd.DataFrame(\n",
        "    {\n",
        "        'name': [x.split('/')[-1] for x in test_path],\n",
        "        'label': pred.argmax(1)\n",
        "})\n",
        "\n",
        "# 生成提交结果\n",
        "submit = submit.sort_values(by='name')\n",
        "submit.to_csv('submit7.csv', index=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4XAo9RAnl4u"
      },
      "outputs": [],
      "source": [
        "model.train()\n",
        "train_loss = 0.0\n",
        "model = model.to(device)\n",
        "for i, (input, target) in enumerate(train_loader):\n",
        "    input = input.to(device)\n",
        "    target = target.to(device)\n",
        "    output = model(input)\n",
        "    loss = criterion(output, target)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RS3Z_iuvnl4v"
      },
      "outputs": [],
      "source": [
        "epochs=4\n",
        "model=model101\n",
        "optimizer = torch.optim.AdamW(model.parameters(), 0.0001)\n",
        "for epoch in range(epochs):\n",
        "    train_loss = train(train_loader, model, criterion, optimizer)\n",
        "    val_acc,val_loss = validate(val_loader, model, criterion)\n",
        "    print(\n",
        "        \"epoch {} :\".format(epoch + 1), \"train_loss:\", train_loss, 'val_loss:', val_loss,\"val_f1:\", val_acc\n",
        "    )\n",
        "    epochs += 1\n",
        "torch.save(model.state_dict(),\"./model/baseline_test.pth\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "abJxaHNNnl4v"
      },
      "outputs": [],
      "source": [
        "epochs=3\n",
        "model101= XunFeiNet().to(device)\n",
        "model=model101\n",
        "optimizer = torch.optim.AdamW(model.parameters(), 0.0001)\n",
        "for epoch in range(epochs):\n",
        "    train_loss = train(train_loader, model, criterion, optimizer)\n",
        "    val_acc,val_loss = validate(val_loader, model, criterion)\n",
        "    print(\n",
        "        \"epoch {} :\".format(epoch + 1), \"train_loss:\", train_loss, 'val_loss:', val_loss,\"val_f1:\", val_acc\n",
        "    )\n",
        "    epochs += 1\n",
        "torch.save(model.state_dict(),\"./model/baseline_101.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IzidOE9Xnl4w"
      },
      "outputs": [],
      "source": [
        "pred = None\n",
        "model=model101\n",
        "model.load_state_dict(torch.load(\"./model/baseline_test.pth\"))\n",
        "for _ in range(3):\n",
        "    if pred is None:\n",
        "        pred = predict(test_loader, model)\n",
        "    else:\n",
        "        pred += predict(test_loader, model)\n",
        "    print(_+1)\n",
        "model.load_state_dict(torch.load(\"./model/baseline_101.pth\"))\n",
        "for _ in range(3):\n",
        "    if pred is None:\n",
        "        pred = predict(test_loader, model)\n",
        "    else:\n",
        "        pred += predict(test_loader, model)\n",
        "    print(_+1)\n",
        "model=model50\n",
        "model.load_state_dict(torch.load(\"./model/baseline_50.pth\"))\n",
        "for _ in range(3):\n",
        "    if pred is None:\n",
        "        pred = predict(test_loader, model)\n",
        "    else:\n",
        "        pred += predict(test_loader, model)\n",
        "    print(_+1)\n",
        "submit = pd.DataFrame(\n",
        "    {\n",
        "        'name': [x.split('/')[-1] for x in test_path],\n",
        "        'label': pred.argmax(1)\n",
        "})\n",
        "\n",
        "# 生成提交结果\n",
        "submit = submit.sort_values(by='name')\n",
        "submit.to_csv('submit6.csv', index=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kW-5kQT0nl4w"
      },
      "outputs": [],
      "source": [
        "model = models.resnet101(weights=models.ResNet101_Weights.DEFAULT)\n",
        "model.fc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PXtoK8WTnl4x"
      },
      "outputs": [],
      "source": [
        "val_acc = validate(val_loader, model)\n",
        "val_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4kdoBemfnl4x"
      },
      "outputs": [],
      "source": [
        "dataset=XunFeiDataset(train_path[:-1000], train_label[:-1000],\n",
        "            A.Compose([\n",
        "            # A.RandomRotate90(),\n",
        "            A.Resize(256, 256),\n",
        "            A.RandomCrop(224, 224),\n",
        "            # A.HorizontalFlip(p=0.5),\n",
        "            # A.RandomBrightnessContrast(p=0.5),\n",
        "            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
        "        ])\n",
        "        )\n",
        "train_dataset, test_dataset=torch.utils.data.random_split(dataset=dataset,lengths=[0.9,0.1],generator=torch.Generator().manual_seed(42))\n",
        "train_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z6LzTBHanl4z"
      },
      "outputs": [],
      "source": [
        "model.load_state_dict(torch.load(\"./model/baseline_test.pth\"))\n",
        "val_acc,val_loss = validate(val_loader, model, criterion)\n",
        "print(val_loss)\n",
        "len(val_loader.dataset)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}